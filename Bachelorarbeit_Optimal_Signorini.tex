% !TeX spellcheck = en_US 
%\documentclass[headsepline,footsepline,footinclude=false,fontsize=11pt,paper=a4,listof=totoc,bibliography=totoc,BCOR=12mm,DIV=12]{scrbook} % two-sided
\documentclass[headsepline,footsepline,footinclude=false,oneside,fontsize=11pt,paper=a4,listof=totoc,bibliography=totoc]{scrbook} % one-sided

%\documentclass[english,a4paper,12pt,oneside]{scrbook}
\usepackage[utf8]{inputenc}
\usepackage[german, english]{babel}
\usepackage{amsmath, amsthm, amssymb}
\allowdisplaybreaks
\usepackage{aligned-overset}
\usepackage{pgfplots}
% and optionally (as of Pgfplots 1.3):
\pgfplotsset{compat=newest}
\pgfplotsset{plot coordinates/math parser=false}
\newlength\figureheight
\newlength\figurewidth 
\usepackage{theoremref}
\usepackage{commath}
\usepackage{mathtools}
\usepackage{aligned-overset}
\usepackage{marvosym}
%\usepackage{graphics}
\usepackage{graphicx} 
\usepackage[%
backend=biber,
url=false,
style=numeric,
maxnames=4,
minnames=3,
maxbibnames=99,
sorting=none,
giveninits,
uniquename=init]{biblatex} % TODO: adapt citation style
\usepackage{microtype}
\usepackage{bera}
\usepackage{xcolor}
\usepackage{siunitx}
\usepackage{xfrac}
\usepackage{epigraph}
\setlength{\epigraphwidth}{0.7\textwidth}
\renewcommand{\epigraphsize}{\footnotesize}
\usetikzlibrary{patterns}


\bibliography{Bibliography}
\setkomafont{disposition}{\normalfont\bfseries} % use serif font for headings
\linespread{1.05} % adjust line spread for mathpazo font

\newenvironment{changemargin}[2]{%
	\begin{list}{}{%
			\setlength{\topsep}{0pt}%
			\setlength{\leftmargin}{#1}%
			\setlength{\rightmargin}{#2}%
			\setlength{\listparindent}{\parindent}%
			\setlength{\itemindent}{\parindent}%
			\setlength{\parsep}{\parskip}%
		}%
		\item[]}
	{\end{list}}

\makeatletter
\makeatletter
\newcommand{\mytag}[2]{%
	\text{#1}%
	\@bsphack
	\protected@write\@auxout{}%
	{\string\newlabel{#2}{{#1}{\thepage}}}%
	\@esphack
}
\makeatother

\makeatletter
\newcommand{\mytaghr}[2]{%
	\text{#1}%
	\@bsphack
	\begingroup
	\@onelevel@sanitize\@currentlabelname
	\edef\@currentlabelname{%
		\expandafter\strip@period\@currentlabelname\relax.\relax\@@@%
	}%
	\protected@write\@auxout{}{%
		\string\newlabel{#2}{%
			{#1}%
			{\thepage}%
			{\@currentlabelname}%
			{\@currentHref}{}%
		}%
	}%
	\endgroup
	\@esphack
}
\makeatother

\makeatletter
\newcommand*{\centerfloat}{%
	\parindent \z@
	\leftskip \z@ \@plus 1fil \@minus \textwidth
	\rightskip\leftskip
	\parfillskip \z@skip}
\makeatother

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

% Define TUM corporate design colors
% Taken from http://portal.mytum.de/corporatedesign/index_print/vorlagen/index_farben
\definecolor{TUMBlue}{HTML}{0065BD}
\definecolor{TUMSecondaryBlue}{HTML}{005293}
\definecolor{TUMSecondaryBlue2}{HTML}{003359}
\definecolor{TUMBlack}{HTML}{000000}
\definecolor{TUMWhite}{HTML}{FFFFFF}
\definecolor{TUMDarkGray}{HTML}{333333}
\definecolor{TUMGray}{HTML}{808080}
\definecolor{TUMLightGray}{HTML}{CCCCC6}
\definecolor{TUMAccentGray}{HTML}{DAD7CB}
\definecolor{TUMAccentOrange}{HTML}{E37222}
\definecolor{TUMAccentGreen}{HTML}{A2AD00}
\definecolor{TUMAccentLightBlue}{HTML}{98C6EA}
\definecolor{TUMAccentBlue}{HTML}{64A0C8}

\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}
\newtheorem{corollary}{Corollary}

\setlength{\parindent}{0em}
\setlength{\parskip}{0.8em}

\begin{document}

% Titelseite
\pagestyle{empty}       % keine Seitennummer
  \parbox{1.5cm}{\resizebox*{110pt}{!}{\includegraphics{tum.pdf}}}\hspace{310pt}%
  \parbox{1.5cm}{\resizebox*{90pt}{!}{\includegraphics{FAK_MA_CMYK.pdf}}}%
\vspace*{1.5cm}
\begin{center}
{\huge \MakeUppercase{Department of Mathematics}} 
\\
\vspace*{5mm}
{\large \MakeUppercase{Technische Universität München} }
\\
\vspace*{2cm}
{\huge {\textbf{{Optimal-Order A-Priori Error Estimates} \\ for the
One-Dimensional\\ Signorini Problem}\par}}
\vspace*{2cm}
{\Large Bachelor's Thesis}\linebreak \\ 
{\Large by}\linebreak \\
{\Large Adrián Löwenberg Casas}\\
\vspace*{1.8cm}
{\large 
\begin{tabular}{ll}
Supervisor: & Prof. Dr. Boris Vexler\\
Advisor: & Dr. Constantin Christof\\
Submission Date: & 15th September 2020
\end{tabular}
}
\end{center}
\newpage    % Seitenwechsel

% Seite 2
\thispagestyle{empty}
\vspace*{0.70\textheight}
\noindent
I confirm that this bachelor’s thesis in mathematics is my own work and I have
documented all sources and material used.

\vspace{30mm}
Munich,\hspace{80mm} Adrián Löwenberg Casas
\enlargethispage{10\baselineskip}
\newpage

\addcontentsline{toc}{chapter}{Acknowledgments}
\thispagestyle{empty}

\vspace*{20mm}

\begin{center}
	{\usekomafont{section} Acknowledgments}
\end{center}

\vspace{10mm}
% Seitennummerierung römisch
\pagenumbering{roman}
% Kopfzeilen (automatisch erzeugt)
\pagestyle{headings}
Firstly, I would like to thank my advisor Dr. Constantin Christof for his valuable and patient help and comments, especially during the difficult circumstances of the Covid-19 pandemic.

A warm thank you goes to my mentor and dear friend Jonas Graw for pushing my to pursue mathematics, as well as to my friend Jonas P.-Hennig, for actually making it possible.
\newpage

% Seite 3
\addcontentsline{toc}{chapter}{Abstract}
\chapter*{}
\vspace*{-2.2cm}
\section*{Zusammenfassung auf Deutsch}
\begin{otherlanguage}{german}
	In dieser Arbeit werden wir das eindimensionale Signorini-Problem behandeln. Nach der Formulierung des Problems werden wir ausführlich seine schwache Formulierung herleiten in Form einer Variationsungleichung sowie eines konvexen Minimierungsproblem in einem Hilbertraum, einer der eindimensionalen Sobolev-Räumen, die wir auf zwei Arten einführen werden. Diese Formulierung wird ein Beweis der Existenz und Eindeutigkeit einer Lösung erlauben.
	
	Wir werden die Finite-Elemente-Methode zur numerischen Approximation von Lösungen zum Problem betrachten. Diese Methode erlaubt eine sehr ähnliche Formulierung des Problems, obgleich in einem endlichdimensionalen Raum. Danach werden wir die Fundiertheit dieser Konstruktion analysieren durch Herleitung von a-priori Konvergenzordnungen optimaler Ordnung für den Fehler der Approximationen in verschiedenen Normen, welche wir zum Schluss mit zwei empirischen Beispiele mittels einer MATLAB Implementierung der Methode testen werden.
\end{otherlanguage}

\section*{Abstract in English}
In this paper we will handle the one-dimensional Signorini problem. After formulating the problem, we will thoroughly derive its weak formulation as a variational inequality and as a convex minimization problem in a Hilbert Space, one of the one-dimensional Sobolev spaces, which we will introduce in two ways. This formulation will allow for the proof of existence and uniqueness of a solution. 

We will consider the finite element method to approximate solutions to the problem numerically. This method allows for a very similar formulation of the problem, albeit in a finite dimensional space. Then, we will analyze the soundness of this construction by deriving optimal-order a-priori convergence orders for the error of the approximations in several norms which we will lastly test with two empirical examples with a MATLAB implementation of the method.



%Seite 4
\newpage
\microtypesetup{protrusion=false}
\tableofcontents{}
\microtypesetup{protrusion=true} 


\pagenumbering{arabic}  % Nummerierung der Seiten in 'arabisch' % neues Kapitel mit Namen "Introduction"
\chapter{History and Background}  \setcounter{page}{1}   % setzt Seitenzaehlung auf 1
\epigraph{``Il mio discepolo Fichera mi ha dato una grande soddisfazione'' [My disciple Fichera gave me a great contentment] \\
	``Ma Lei ne ha avute tante, Professore, durante la Sua vita'' [But you, Professor, have had so many throughout your life], \textit{replied Doctor Aprile, but then Signorini replied again:} \\
	``Ma questa è la più grande.'' [But this is the greatest one] \textit{And those were his last words.}
}{\textit{Antonio Signorini speaking with his doctor Damiano Aprile about his student Gaetano Fichera having solved his probelm \cite{Fichera95}.}}

The original Signorini problem dates back to 1959, when Antonio Signorini (Rome, 1888) posed it in a course he taught on the \textit{Instituto Nazionale di Alta Matematica} \cite{signorini59}. The problem consisted of finding the elastic equilibrium configuration of a spherical elastic body on a rigid frictionless plane which satisfied a set of ambiguous boundary conditions. Signorini used the term ``ambiguous'' himself, since it was not known a-priori which of two alternative sets of boundary conditions involving equalities and inequalities were to be satisfied at any given point. 

One of his students at the time, Gaetano Fichera, who would eventually solve the problem before Signorini's death, to his great satisfaction, attended the course and was intrigued not to find any similar problems in the literature about boundary value problems \cite{Fichera95}. His proof of the existence of a unique solution was published soon after Signorini's death \cite{fichera63}, where the problem was coined ``Signorini's Problem'' in his teacher's honor.

The problem we will analyze here is a simpler, one dimensional version of the problem, where an elastic beam with a two-point contact set on the boundary is subject to a force. The ambiguity of the boundary conditions, as stated by Signorini, refers here to the fact that the beam might or might not be in contact with the surface after applying the force, and therefore the conditions are different in each case.
\pagebreak

\begin{definition}The One Dimensional Signorini Problem \newline
	Let $\Omega \coloneqq (-1,1)$. We denote the boundary of $\Omega$ by $\partial\Omega = \{-1,1\}$ and the normal derivative of $u$ on $\partial\Omega$ by $\partial_nu$, i.e. $\partial_nu(-1) = -u'(-1)$ and $\partial_nu(1) = u'(1)$. Furthermore let $f \in L^2(-1,1)$ be the given forcing term. The One-Dimensional Signorini Problem is defined as follows:
	\begin{align}
	-u'' + u &= f \quad \textnormal{in} \quad \Omega \label{eq:signorini}\\ 
	\textcolor{TUMBlue}{\partial_n u \geq 0},\quad \textcolor{TUMAccentOrange}{u}\, &\textcolor{TUMAccentOrange}{\geq 0},\textcolor{TUMGray}{\quad u\partial_nu = 0} \quad\textnormal{on}\quad \partial \Omega \label{eq:boundary_signorini}
	\end{align}
	The third boundary condition is the ``ambiguous'' one, as either the beam touches the surface $u(\pm1) = 0$, or it has vanishing derivative on the border $\partial_nu(\pm 1) = 0$.
\end{definition}

\begin{figure}[h!]
	\centering
	\begin{tikzpicture}[scale=4.5]
	\draw (-1.25,1) -- (-1,1) -- (-1,0.25);
	\draw (1,0.25) -- (1,1) -- (1.25,1);
	\pattern[pattern=north west lines, pattern color=black] (-1.25,0.25) rectangle (-1,1)
	(1,0.25) rectangle (1.25,1);
	\node[anchor = north] at (-1.125,0.25) {$\vdots$};
	\node[anchor = north] at (1.125,0.25) {$\vdots$};

	\draw[thick] plot [smooth] coordinates{(-1,1) (-0.75,0.9451) (-0.5,0.9719) (-0.25,1.0569) (0,1.1764) (0.25,1.3069) (0.5,1.4258) (0.75,1.5116) (1,1.5443)}; 
	
	\draw[->] (-1,2) to 	(-1,	1.5793);
	\draw[->] (-0.75,2) to 	(-0.75,	1.6592);
	\draw[->] (-0.5,2) to 	(-0.5,	1.7603);
	\draw[->] (-0.25,2) to 	(-0.25,	1.8763);
	%\draw[->] (0,2) to 		(0,2);
	\draw[->] (0.25,2) to 	(0.25,	2.1237);
	\draw[->] (0.5,2) to 	(0.5,	2.2397);
	\draw[->] (0.75,2) to 	(0.75,	2.3408);
	\draw[->] (1,2) to 		(1,		2.4207);
	\node at (0,2) {$f$};
	
	\node at (0,1.30) {$u$};
	
	\node[anchor=south] at (-1,1) {$\scriptstyle \substack{\textcolor{TUMAccentOrange}{u(-1) \geq 0} \\ \textcolor{TUMGray}{u (-1) = 0}}$};
	\draw[->] (-1,1) to	(-0.75,	0.85);
	\node[anchor = north] at (-0.75,	0.85) {$\scriptstyle \textcolor{TUMBlue}{\partial_n u(-1) \geq 0}$};
	
	\node[anchor=south] at (1,1.5433) {$\scriptstyle \textcolor{TUMAccentOrange}{u (1) \geq 0}$};
	\draw[->] (1,1.5433) to	(1.25,	1.5433);
	\node[anchor = west] at (1.25,	1.5433) {$\scriptstyle \substack{\textcolor{TUMBlue}{\partial_n u(1) \geq 0} \\ \textcolor{TUMGray}{\partial_nu (1) = 0}}$};
	
	\node[anchor = north] at (0, 0)   {\scriptsize 0};
	\node[anchor = north] at (1, 0)   {\scriptsize 1};
	\node[anchor = north] at (-1, 0)   {\scriptsize -1};
	\node[anchor = east] at (-1.5, 1)   {\scriptsize 0};

	\draw[<->] (-1, 0) to (1, 0);
	\draw[<->] (-1.5, 0) to (-1.5, 1);
	\end{tikzpicture}
	\caption[Visualization of the Signorini Problem]{Rough visualization of the solution $u$ of the Signorini problem with a right hand side $f = \sin$, satisfying different boundary conditions on each contact point.}
\end{figure}

\chapter{Solvability of the Signorini Problem}

The classical formulation \eqref{eq:signorini} of the Signorini problem has an intuitive physical meaning, but it is not suitable for the mathematical analysis of the existence of a solution. We will work towards the so-called weak formulation of the problem, which will allow for the usage of general functional analysis techniques, as well as some from the calculus of variations. To that end, we will firstly define the Sobolev Banach and Hilbert Spaces.

\section{Sobolev Spaces}
\begin{definition}One Dimensional Sobolev-Spaces \newline
	Let $p\in [1,\infty]$. For $k \in \mathbb{N}_0$, the Sobolev-Space $W^{k,p}(a,b)$ is defined as follows:
	\begin{align}
	\begin{split}
	W^{0,p}(a,b) &\coloneqq L^p(a,b) \\
	W^{k,p}(a,b) &\coloneqq \big\{ v \in W^{k-1,p}(a,b) \,|\, \exists w \in W^{k-1,p}(a,b), c\in\mathbb{R}. \\
	& \phantom{\coloneqq \big\{\,\,} v(t) =  c + \int_a^t w(\tau) \dif \tau \quad \text{f.a.a.} \,\, t\in (a,b)  \big\}
	\end{split}
	\end{align}
	For $v\in W^{k,p}$ the function $w$ in the definition is called the weak derivative of $v$, and in this context the notation $v' \coloneqq w$ is useful. Because of the Fundamental Theorem of Calculus, one can easily see that if $v$ has a classical derivative, then it coincides with the weak one. One can naturally expand this notation to the $k$th weak derivative $v^{(k)}$.
	
	$W^{k,p}(a,b)$ is a Banach Space with the following norm:
	\begin{equation}
	\norm{v}_{W^{k,p}(a,b)} \coloneqq \left(\sum\limits_{i = 0}^k \norm{v^{(i)}}_{L^p(a,b)}^p\right)^{1/p} \label{eq:sobolev1_norm}
	\end{equation}
	In the case of $p=2$ the notation $H^k(a,b) \coloneqq W^{k,2}(a,b)$ is used. This space is a Hilbert Space with the following inner product:
	\begin{equation}
	(v,z)_{H^k(a,b)} \coloneqq \sum_{i=0}^{k} (v^{(k)}, z^{(k)})_{L^2([a,b])} = \sum_{i=0}^{k} \int_a^b v^{(k)}(\tau)z^{(k)}(\tau) \dif \tau \label{eq:sobolev1_inner_product}
	\end{equation}
	These results can be found in the literature, for example in Mitrović and Žubrinić \cite[Chapter 5, Section 2, Remark 2]{mitrovic1997fundamentals}.
\end{definition}

This definition of the Sobolev Spaces, although the most simple, is not the one most commonly found in the literature. The usual way to define a weak derivative, and therefore the Sobolev Spaces is through testing with infinitely differentiable functions with compact support. This alternative definition will be useful down the line, as it is very analogous to the variational inequalities we will be considering:

\begin{lemma} Equivalent Characterization of the Sobolev Spaces \newline
	Let $C_c^\infty([a,b])$ be the space of the infinitely differentiable functions with compact support inside $(a,b)$. Note that this implies that these functions disappear on the boundary. Let $k \in \mathbb{N}_0$. Let $v$ be a locally integrable function. A locally integrable function $w$, which fulfills:
	\begin{equation} \label{eq:weak_derivative_through_testing}
	\int_a^b u \varphi^{(k)} \dif x = \int_a^b w \varphi \dif x \quad \forall \varphi \in C_c^\infty([a,b])
	\end{equation}
	or, equivalently:
	\begin{equation}
		(u,\varphi^{(k)})_{L^2([a,b])} = (w,\varphi)_{L^2([a,b])} \quad \forall \varphi \in C_c^\infty([a,b])
	\end{equation}
	is the $k$th weak derivative of $v$, i.e. $v^{(k)} = w$. With this definition one can also see through partial integration that in the case that a classical derivative exists, it is also the weak derivative.
	
	 furthermore $p\in [1,\infty]$. For $k \in \mathbb{N}_0$ and $p \in \mathbb{N}$, the Sobolev Space $W^{k,p}(a,b)$ is the space of all $L^p([a,b])$-functions whose $i$th partial derivative, defined in the above sense, has a finite $L^p$-norm, for all $i \in \{0,\dots,k\}$. 
	
	
	Again, this definition can be found in the literature, for example in Kurdilla and Zabarankin \cite[Definition 3.2.3]{kurdila2005convex}.
\end{lemma}

The Sobolev Spaces are subspaces of the $L^p$-spaces, therefore their elements are equivalence classes of Lebesgue-almost everywhere equal functions. In order to formulate the Signorini Problem weakly, we however need to enforce the boundary conditions, which are defined on the two points of $\partial \Omega$. We will show that in the case of $H^k([a,b])$, there is always a canonical representative in $C^{k-1}([a,b])$.


\begin{lemma}\thlabel{thm:sobolev_embedding_1} Sobolev Embedding for $H^1([a,b])$ \newline
	For every $u \in H^1([a,b])$ there exists a $v \in C^{0,\frac{1}{2}}([a,b])$, where $C^{0,\frac{1}{2}}$ is a Hölder-Space with Hölder-continuity constant $\frac{1}{2}$, such that $u = [v \vert_{(a,b)}]$. As an embedding, this can be written as:
	\begin{equation} \label{eq:sobolev1_embedding}
	H^1([a,b]) \hookrightarrow C^{0,\frac{1}{2}}([a,b])
	\end{equation}
	\begin{proof}
	Let $u \in H^1([a,b])$, $a \leq x_1 < x_2 \leq b$, and let $u = [v]$ such that $v(x) = v_0 + \int_a^x w(\tau) \dif \tau$. We will show that $v$ is $\frac{1}{2}$-Hölder-continuous:
	\begin{align*}
	\vert v(x_1) - v(x_2) \vert &= \left\vert \int_{a}^{x_1} w(\tau) \dif \tau - \int_{a}^{x_2} w(\tau) \dif \tau \right\vert \\
	&= \left\vert \int_{x_1}^{x_2} w(\tau) \dif \tau \right\vert \overset{\textnormal{Hölder}}{\leq} \Vert w \Vert_{L^2} \left\vert \int_{x_1}^{x_2} 1 \dif \tau \right\vert ^{\frac{1}{2}} \\
	& = \Vert w \Vert_{L^2} \vert x_1 - x_2 \vert ^{\frac{1}{2}}
	\end{align*}
	\end{proof}
\end{lemma}

\begin{corollary} Sobolev Embedding for $H^k(a,b)$ \newline
		For every $u \in H^k(a,b)$ there exists a $v \in C^{k-1}([a,b])$, such that $u = [v \vert_{(a,b)}]$. As an embedding, this can be written as:
	\begin{equation}
	H^k(a,b) \hookrightarrow C^{k-1}([a,b])
	\end{equation}
	\begin{proof}
		This follows directly from \thref{thm:sobolev_embedding_1} and the Fundamental Theorem of Calculus.
	\end{proof}
\end{corollary}
	

Using these statements, we will henceforth identify elements of $H^k$ with their continuous representatives, allowing for the following statements involving pointwise conditions on elements of this space to be well-defined.

\section{Weak Formulation}

\begin{theorem} Weak Formulation of the Signorini Problem\newline
	Let $K \coloneqq \left\{ v\in H^1(\Omega) \,|\, v(-1) \geq 0, v(1) \geq 0 \right\}$. Then a solution $u \in K$ of the following variational inequality is a weak solution of \eqref{eq:signorini}:
	\begin{equation}
	\int_{-1}^1 u'(v'-u') + u(v-u) - f(v-u) \dif x \geq 0 \quad \forall v \in K \label{eq:variational_inequality}
	\end{equation}
	Taking the definition of the inner product in the space $H^1$ into account \eqref{eq:sobolev1_inner_product}, this can be equivalently written as:
	\begin{equation}
	(u,v-u)_{H^1(\Omega)} \geq (f,v-u)_{L^2(\Omega)} \quad \forall v \in K
	\end{equation}
	\begin{proof}
	Consider the differential form of the Signorini Problem \eqref{eq:signorini} \eqref{eq:boundary_signorini} and suppose $u \in C([-1,1])$ solves it classically. Furthermore let $v \in K$. Multiplying \eqref{eq:signorini} by the test function $v-u$ and integrating over the domain $\Omega$ yields:
	\begin{align*}
	&\int_{-1}^1 {(-u'' + u - f)(v - u)\dif x} = 0  \\
	\Longleftrightarrow &\int_{-1}^1 -u''(v-u) \dif x + \int_{-1}^1 u(v-u) - f (v-u) \dif x =0
	\end{align*}
	The right term is already in the required form. We now integrate the left term by parts to remove the second derivative:
	\begin{align}
	\Longleftrightarrow \underbrace{\left. -u'(v-u) \right|_{-1}^{1}}_{\textnormal{(}\mytag{*}{eq:boundary_int_term}\textnormal{)}} + \int_{-1}^1{u'(v'-u') + u(v-u) - f(v-u) \dif x} = 0 \label{eq:variational_equality}
	\end{align}
	We resolve the boundary term from the integration by parts with the boundary conditions on the Signorini Problem \eqref{eq:boundary_signorini}:
	\begin{align*}
	\eqref{eq:boundary_int_term} &= -u'(1)v(1) + \underbrace{u'(1)u(1)}_{\overset{\eqref{eq:boundary_signorini}}{=} 0} + u'(-1)v(-1) - \underbrace{u'(-1)u(1)}_{\overset{\eqref{eq:boundary_signorini}}{=} 0} \\
	&= -u'(1)v(1) + u'(-1)v(-1) \overset{\eqref{eq:boundary_signorini}}{\leq} 0
	\end{align*}
	Enforcing this inequality on \eqref{eq:variational_equality} results in the desired variational inequality \eqref{eq:variational_inequality}.
	
	We now observe that the derived formulation of the problem does not need the assumption of $u \in C([-1,1])$ and can be more generally defined for $u \in K$. Hence this is the weak formulation of the problem.
	
	\end{proof}
\end{theorem}

We will now formulate this problem as a convex minimization problem in the Sobolev Banach Space. This will be useful when considering the existence and uniqueness of solutions, as it will allow for the use of standard techniques of convex optimization.

\begin{lemma} The functional to minimize is convex \newline
	Define the functional $F$ as follows:
	\begin{equation}
			F(v) \coloneqq \int_{-1}^1{\frac{1}{2} (v')^2 + \frac{1}{2} v^2 - fv \dif x}
	\end{equation}
	This functional is uniformly (therefore strictly) convex.
	\begin{proof}
		Firstly, we note that $F(v) = \frac{1}{2} \norm{v}_{H^1}^2 - (f,v)_{L^2} $. It therefore suffices to show that $v \mapsto \norm{v}_{H^1}^2$ is uniformly convex, as affine transformations do not alter it. We will show that this is true for every scalar-product induced norm in a space $(X, (\cdot,\cdot))$. To that avail, let $v,w \in X$, $v \neq w$ and $\lambda \in (0,1)$:
		\begin{align*}
		(1-\lambda)\norm{v}^2 &+ \lambda\norm{w}^2 - \norm{(1-\lambda)v - \lambda w}^2 \\
		& = (1-\lambda) (v,v) + \lambda (w,w) - (((1-\lambda)v,(1-\lambda)v) + 2((1-\lambda)v, \lambda w)+ (\lambda w, \lambda w)) \\
		&= (1-\lambda) (v,v) - (1-\lambda)^2 (v,v) - 2 (1-\lambda)\lambda(v,w) + \lambda (w,w) - \lambda^2 (w,w) \\
		&= (1-\lambda)(1-(1-\lambda))(v,v)- 2 (1-\lambda)\lambda(v,w) + \lambda (1-\lambda)(w,w) \\
		&= \lambda(1- \lambda)((v,v) - 2(v,w)+(w,w)) = \lambda(1-\lambda)(v-w,v-w) \\
		&= \lambda(1-\lambda)\norm{v-w}^2
		\end{align*}
		This is exactly the definition with modulus of convexity $\phi(\alpha) = \alpha ^2$.
	\end{proof}
\end{lemma}

We will need a slightly more general statement about variational inequalities of this form, as we will solve a similar problem but with different boundary conditions for the $L^2$-error estimate. To that end we define the generalized boundary condition set $K^0$.

\begin{definition} Generalized Boundary Conditions \newline
	A convex subset of functions $K^0 \subseteq H^1(\Omega)$ is said to fulfill the generalized boundary conditions if it is any of the following:
	\begin{align}
	\begin{split} \label{eq:generalized_boundary_condition_set}
	K^0 \in \big\{ &H^1(\Omega), \\
	\{ &v\in H^1(\Omega) \,|\, v(-1) \geq 0\}, \\
	\{ &v\in H^1(\Omega) \,|\, v(1) \geq 0 \}, \\
	\{ &v\in H^1(\Omega) \,|\, v(-1) \geq 0, v(1) \geq 0\}
	\big\}
	\end{split}
	\end{align}
\end{definition}

Note that, in particular, $K$ fulfills the generalized boundary conditions, and therefore all statements regarding a $K^0$ are also valid for $K$.

\begin{theorem} Equivalent formulation as a convex minimization problem\newline
	$u \in H^1(\Omega)$ solves the following minimization problem if and only if it solves a variational inequality of the form \eqref{eq:variational_inequality}, but with a test set $K^0$:
	\begin{equation} \label{eq:minimization_problem}
	\min_{v \in K^0} F(v)
	\end{equation}
	The functional $F$ can be understood as the energy functional. This is a convex problem.
	\begin{proof}
	We start with the slight modification of the variational inequality \eqref{eq:variational_inequality}:
	\begin{align*}
		&\int_{-1}^1 u'(v'-u') + u(v-u) - f(v-u) \dif x \geq 0 \quad \forall v \in K^0 \\
		\Longleftrightarrow & \int_{-1}^1 u'v' - (u')^2 + uv - u^2 - fv - fu \dif x \geq 0 \quad \forall v \in K^0 \\
	\end{align*}
	We will now use the Young inequality to bound the integral from above:
	\begin{equation}
	ab \leq \frac{1}{2} a^2 + \frac{1}{2} b^2 \label{eq:binomial_inequality}
	\end{equation}
	This follows directly from the binomial formula. Using it on the variational inequality yields the following:
	\begin{align*}
	&\int_{-1}^1 \frac{1}{2} (u')^2 + \frac{1}{2} (v')^2 - (u')^2 + \frac{1}{2} u^2 + \frac{1}{2} v^2 - u^2 - fv - fu \dif x \geq 0 \quad \forall v \in K^0 \\
	\Longleftrightarrow & \int_{-1}^1 \frac{1}{2} (v')^2 + \frac{1}{2} v^2 - fv \dif x \geq \int_{-1}^1 \frac{1}{2} (u')^2 + \frac{1}{2} u^2 - fu \dif x \quad \forall v \in K^0
	\end{align*}
	Or, equivalently:
	\begin{equation*}
	F(v) \geq F(u) \quad \forall v \in K^0
	\quad \Longleftrightarrow \quad u = \argmin_{v \in K^0} F(v)
	\end{equation*}
	
	For the other direction of the proof let $u$ be a solution of \eqref{eq:minimization_problem}. As $u$ is a minimizer, it follows for every $t \geq 0$ and every $v \in K^0$:
	\begin{equation*}
	\frac{F(u+t(v-u)) - F(u)}{t} \geq 0
	\end{equation*}
	Starting from there, we will derive the variational inequality by taking the limit $t\rightarrow 0$:
	\begingroup
	\allowdisplaybreaks
	\begin{align*}
	&\lim\limits_{t \rightarrow 0} \frac{1}{t} \Bigg( 
	\int_{-1}^{1} \frac{1}{2} (u'+t(v'-u'))^2 + \frac{1}{2} (u+t(v-u))^2 - f(u+t(v-u))\\
	 &\phantom{\lim\limits_{t \rightarrow 0} \frac{1}{t} \Bigg( 
		\int_{-1}^{1}} -\frac{1}{2}(u')^2  - \frac{1}{2}u^2 + fu \dif x \Bigg) \\
	= &\lim\limits_{t \rightarrow 0} \int_{-1}^1 \frac{1}{2} t(v'-u')^2 + u'(v'-u') + \frac{1}{2}t(v-u)^2 + u(v-u) -f(v-u) \dif x \\
	= & 	\int_{-1}^1 u'(v'-u') + u(v-u) - f(v-u) \dif x \geq 0 \quad \forall v \in K^0
	\end{align*}
	\endgroup
	This is exactly the required form, and the equivalence of the two formulations follows.
	\end{proof}
\end{theorem}

\section{Existence and Properties of the Solution}
\begin{theorem}\thlabel{thm:unique_solution_signorini} Existence of a unique solution to the Signorini Problem \newline
	A minimization problem of the form \eqref{eq:minimization_problem}, such as the weak formulation of the Signorini Problem, has a unique solution $u \in H^1(\Omega)$
	\begin{proof}
		We will prove this using the Direct Method in the Calculus of Variations. In a first step, we will show that the functional $F$ is bounded from below. To this avail define $\{v_n\}_{n\in \mathbb{N}} \subset H^1(\Omega)$ such that: 
		\begin{equation*}
		v_n(\pm 1) \geq 0\textnormal{,} \quad \lim\limits_{n \rightarrow \infty} F(v_n) = \inf_{v ?in K^0} F(v)
		\end{equation*} 
		Then there exists a constant $C$ such that:
		\begin{align*}
		C \geq F(v_n) \overset{\eqref{eq:minimization_problem}}&{=} \frac{1}{2} \norm{v_n}_{H^1}^2 - (f,v)_{L^2} = \frac{1}{2} \norm{v_n}_{H^1}^2 - \left(4f,\frac{1}{4}v\right)_{L^2} \\
		&\geq \frac{1}{2}\norm{v_n}_{H^1}^2 - \frac{1}{4} \norm{v_n}_{L^2}^2 - 4 \norm{f}_{L^2}^2 \\
		&\geq \frac{1}{4} \norm{v_n}_{H^1}^2 - 4 \norm{f}_{L^2}^2
		\end{align*}
		\begin{equation} \label{eq:H1_bound_by_f}
		\Longrightarrow \norm{v_n}_{H^1}^2 \leq 4\left(C + 4 \norm{f}_{L^2}^2\right) \eqqcolon \tilde{C}
		\end{equation}
		The infimal sequence is bounded and therefore also the functional. Now we will use the Functional Analysis result, that a bounded sequence in a Hilbert Space has a weakly convergent subsequence. This can be found for example in Royden and Fitzpatrick \cite[Chapter 16, Theorem 6]{royden2010real}. On top of that, we use that a convex, continuous functional is weakly lower semi-continuous, as can be found in Kurdila and Zabarankin \cite[Theorem 7.2.5]{kurdila2005convex}. 
		
		Let $\{\tilde{v}_n\}_{n\in \mathbb{N}} $ be the weakly convergent subsequence such that $\tilde{v}_n \rightharpoonup u$. Then the claim follows, by definition of weak convergence and lower semi-continuity:
		\begin{equation*}
		\inf_{v \in K^0} F(v) =  \liminf\limits_{n\rightarrow\infty} F(\tilde{v}_n) \geq F(u)
		\end{equation*}
		
		The uniqueness of the solution follows from the strict convexity of the functional \cite{aubin79}.
	\end{proof}
\end{theorem}

Now we will see that without any further assumptions, the weak solution to the Signorini Problem is actually in $H^2$ and its norm is bound by the right hand side $f$.

\begin{theorem} $H^2$-regularity of the solution to the Signorini Problem \newline
	Let $u \in H^1(\Omega)$ be the solution to \eqref{eq:minimization_problem}. Then the following holds for $C \in \mathbb{R}_{>0}$:
	\begin{align}
		u &\in H^2(\Omega) \label{eq:H2_regularity} \\
		\norm{u}_{H^2(\Omega)} &\leq C\norm{f}_{L^2(\Omega)} \label{eq:H2_norm_bound_by_f} 
	\end{align}
	\begin{proof}
		$u \in H^1(\Omega)$ be the solution to \eqref{eq:minimization_problem}. Then it is also the solution of \eqref{eq:variational_inequality} and therefore fulfills the following:
		\begin{equation*}
		(u,v-u)_{H^1(\Omega)} \geq (f,v-u)_{L^2(\Omega)} \quad \forall v \in K
		\end{equation*}
		Let $\varphi \in C_c^\infty(\Omega)$, and let $v_1 = u + \varphi$ and $v_2 = u - \varphi$. Since $u$ is a solution and therefore fulfills the boundary conditions and $\varphi$ disappears on the boundary, $v_1,v_2 \in K$. Testing with both in the variational inequality, as $\varphi$ was arbitrary, yields the following:
		\begin{equation*}
			(u, \varphi)_{H^1(\Omega)} = (f, \varphi)_{L^2(\Omega)} \quad \forall \varphi \in C_c^\infty(\Omega)
		\end{equation*}
		We will transform this to get the characterization of $H^2$-regularity \eqref{eq:weak_derivative_through_testing}:
		\begin{align*}
		&(u,\varphi)_{H^1(\Omega)} \overset{\eqref{eq:sobolev1_inner_product}}{=} (u,\varphi)_{L^2(\Omega)} + (u',\varphi')_{L^2(\Omega)} = (f, \varphi)_{L^2(\Omega)} \quad \forall \varphi \in C_c^\infty(\Omega) \\
		\Longleftrightarrow \quad &(u',\varphi')_{L^2(\Omega)} = (f-u,\varphi)_{L^2(\Omega)} \quad \forall \varphi \in C_c^\infty(\Omega)
		\end{align*}
		Which means that $u' \in H^1(\Omega)$, with $(u')' = f-z$, implying $u \in H^2(\Omega)$.
		
		Using $u'' = f-z$, the characterizing equation of the weak derivative and \eqref{eq:H1_bound_by_f} we obtain:
		\begin{align*}
			\norm{u}_{H^2(\Omega)}\overset{\eqref{eq:sobolev1_norm}}{=} \norm{u}_{H^1(\Omega)} + \norm{u''}_{L^2(\Omega)} 
			\overset{\substack{\text{Cauchy} \\ \text{Schwarz}}}{\leq} 2\norm{u}_{H^1(\Omega)} + \norm{f}_{L^2(\Omega)} 
			\overset{\eqref{eq:H1_bound_by_f}}{\leq} C \norm{f}_{L^2(\Omega)}
		\end{align*}
	\end{proof}
\end{theorem}


\chapter{Discretization}
We will now consider a discretization of the Signorini Problem. To that end, we will restrict the solution space to the space of so-called finite element functions $V_h$ with a mesh width $h$, which we assume uniform with $h = x_{i+1} - x_i$:
\begin{align}
V_h \coloneqq \{&z \in C([-1,1]) \, | \, \\ 
	&z \,\, \text{piecewise affine subordinate to a partition} \mathmbox{-1}=x_0<x_1<\dots<x_N = 1 \} \nonumber
\end{align} 

\begin{definition} Finite Element Space
We define a basis for the finite element space $V_h$, which are the piecewise linear functions with a bump in the nodes:
\begin{align} \label{eq:basis_finite_element_space}
\begin{split}
V_h = \mathrm{span}\,\Phi_h &= \mathrm{span}\{\varphi_i^h \,|\, i \in \{0,\dots,N\}\} \\
\varphi_i^h (x) &= \begin{cases}
\frac{x-x_{i-1}}{h} & \text{if} \quad x\in [x_{i-1},x_i] \\
\frac{x_{i+1}-x}{h} & \text{if} \quad  x\in [x_i,x_{i+1}] \\
0 & \text{if} \quad  \abs{x-x_i} > h
\end{cases}
\end{split}
\end{align}
\end{definition}

\begin{definition} The Discrete Signorini Problem \newline
	Using this discretization, \eqref{eq:minimization_problem} turns into:
	\begin{equation} \label{eq:discrete_minimization}
	\min_{\substack{v_h \in V_h \\ v_h(-1) \geq 0,\, v_h(1) \geq 0}} F(v_h) = \min_{\substack{v_h \in V_h \\ v_h(-1) \geq 0,\, v_h(1) \geq 0}} \int_{-1}^1{\frac{1}{2} (v_h')^2 + \frac{1}{2} v_h^2 - fv_h \dif x} 
	\end{equation}
	Or, equivalently, as a variational inequality:
	\begin{equation}\label{eq:discrete_variational_inequality}
	u_h \in V_h,\quad (u_h, v_h-u_h)_{H^1(\Omega)} \geq (f, v_h - u_h)_{L^2(\Omega)} \quad \forall v_h \in V_h \cap K
	\end{equation}
	
\end{definition}

We could use the same techniques from the previous section to show that this problem always admits a unique solution. However, it will be much more obvious if we rewrite it in the usual finite dimensional form. This will also be the form used to implement a solver.

\begin{lemma} Matrix Formulation of the Discrete Problem \newline
	Using the basis \eqref{eq:basis_finite_element_space}, we represent the solution function $v_h$ as a linear combination of the basis functions and define $\mathbf{v}$ as the corresponding coefficient vector:
	\begin{equation} \label{eq:basis_representation_v_h}
	v_h(x) = \sum_{i=1}^{N} \mathbf{v}_i \varphi_i^h(x)
	\end{equation}
		The above problem \eqref{eq:discrete_minimization} is then equivalent to:
		\begin{equation} \label{eq:matrix_formulation}
		\min_{\substack{\mathbf{v} \in \mathbb{R}^{(N+1)\times(N+1)}\\ \mathbf{v}_{0} \geq 0,\, \mathbf{v}_N \geq 0}} \frac{1}{2} \mathbf{v}^T (B_1 + B_2) \mathbf{v} - \mathbf{f}^T  \mathbf{v}
		\end{equation}
		We call $B_1$ the \emph{mass matrix} and $B_2$ the \emph{stiffness matrix}. $B_1$, $B_2$ and $\mathbf{f} \in \mathbb{R}^{(N+1)\times(N+1)}$ are defined as follows:
		\begin{align*}
		B_1 &\coloneqq 			h	\begin{pmatrix}
		1/3& 1/6 & & & & & \\
		1/6& 2/3& 1/6 & & & & \\
		& 1/6& 2/3& 1/6 & & & \\
		& & & \ddots & & & \\
		& & & 1/6& 2/3& 1/6 & \\
		& & & & 1/6& 2/3& 1/6 \\
		& & & & & 1/6& 1/3
		\end{pmatrix}, \\
		 B_2 &\coloneqq \frac{1}{h} 			\begin{pmatrix}
		1& -1 & & & & & \\
		-1& 2& -1 & & & & \\
		& -1& 2& -1 & & & \\
		& & & \ddots & & & \\
		& & & -1& 2& -1 & \\
		& & & & -1& 2& -1 \\
		& & & & & -1& 1
		\end{pmatrix} \\
		\\
		\mathbf{f}_i &\coloneqq \phantom{h} \left(\int_{-1}^1 f\varphi_i^h\dif x\right)
		\end{align*}
		Note that only $\mathbf{f}$ changes with the choice of $f$, the stiffness and mass matrices are therefore structural to the problem. In practice, as portrayed in the implementation section, the vector $\mathbf{f}$ can only be approximated numerically with a quadrature of high enough order.
		\begin{proof}
			We will analyze \eqref{eq:discrete_minimization} in three parts. For the mass and stiffness matrices we will decompose the integration interval into the intervals $[t_{i-1},t_i]$, so that we can consider only the two basis functions corresponding to the left and right node of the interval:
			\begin{align*}
				\int_{-1}^1 v_h^2 \dif x &= \sum_{i=1}^{N} \int_{t_{i-1}}^{t_i} (\mathbf{v}_i \varphi_i + \mathbf{v}_{i-1} \varphi_{i-1})^2 \dif x\\
				&= \sum_{i=1}^{N} \int_{t_{i-1}}^{t_i} \left(\frac{x-x_{i-1}}{h}\mathbf{v}_i + \frac{x_i-x}{h}\mathbf{v}_{i-1}\right)^2 \dif x \\
				&= \sum_{i=1}^{N} \int_{t_{i-1}}^{t_i} \left(\frac{x-x_{i-1}}{h}\right)^2 \mathbf{v}_i^2 + 2\left(\frac{x-x_{i-1}}{h}\right) \mathbf{v}_i \left(\frac{x_i-x}{h}\right) \mathbf{v}_{i-1} + \left(\frac{x_i-x}{h}\right)^2 \mathbf{v}_{i-1}^2 \dif x \\
				&= \sum_{i=1}^{N} \frac{1}{3}h \mathbf{v}_i^2 + 2\frac{1}{6}h\mathbf{v}_i \mathbf{v}_{i-1} + \frac{1}{3} h \mathbf{v}_{i-1}^2 \\
				&= h \begin{pmatrix}
					\mathbf{v}_0 \\					
					\mathbf{v}_1 \\
					\vdots \\
					\mathbf{v}_N 
				\end{pmatrix}^T
				\begin{pmatrix}
				1/3& 1/6 & & & & & \\
				1/6& 2/3& 1/6 & & & & \\
				& 1/6& 2/3& 1/6 & & & \\
				& & & \ddots & & & \\
				& & & 1/6& 2/3& 1/6 & \\
				& & & & 1/6& 2/3& 1/6 \\
				& & & & & 1/6& 1/3
				\end{pmatrix}
				\begin{pmatrix}
				\mathbf{v}_0 \\					
				\mathbf{v}_1 \\
				\vdots \\
				\mathbf{v}_N 
				\end{pmatrix} = \mathbf{v}^T B_1 \mathbf{v}
			\end{align*}
			We proceed similarly for the stiffness matrix, here differentiating the two basis functions before integrating $v_h$:
			\begin{align*}
			\int_{-1}^1 (v_h')^2 \dif x &= \sum_{i=1}^{N} \int_{t_{i-1}}^{t_i} \left(\frac{x_i-x_{i-1}}{h}\right)^2 \dif x \\
			&= \sum_{i=1}^{N} \frac{1}{h} \mathbf{v}_i^2 - 2\frac{1}{h} \mathbf{v}_i\mathbf{v}_{i-1} + \frac{1}{h}\mathbf{v}_{i-1}^2 \\
			&= \frac{1}{h} 
			\begin{pmatrix}
			\mathbf{v}_0 \\					
			\mathbf{v}_1 \\
			\vdots \\
			\mathbf{v}_N 
			\end{pmatrix}^T
			\begin{pmatrix}
			1& -1 & & & & & \\
			-1& 2& -1 & & & & \\
			& -1& 2& -1 & & & \\
			& & & \ddots & & & \\
			& & & -1& 2& -1 & \\
			& & & & -1& 2& -1 \\
			& & & & & -1& 1
			\end{pmatrix}
			\begin{pmatrix}
			\mathbf{v}_0 \\					
			\mathbf{v}_1 \\
			\vdots \\
			\mathbf{v}_N 
			\end{pmatrix} = \mathbf{v}^T B_2 \mathbf{v}
			\end{align*}
			Finally, we derive the vector $\mathbf{f}$, for which we will use the linearity of $v_h$ in the third part of \eqref{eq:discrete_minimization}:
			\begin{align*}
				\int_{-1}^1 fv_h \dif x &= \int_{-1}^1 f \sum_{i=0}^{N} \mathbf{v}_i \varphi_i^h \dif x \\
				&= \sum_{i=0}^{N} \mathbf{v}_i \left(\int_{-1}^1 f\varphi_i^h\dif x\right) = \mathbf{f}^T \mathbf{v}
			\end{align*}
		\end{proof}
\end{lemma}

\begin{corollary}\thlabel{thm:ex_uniq_discrete_sol} Existence and Uniqueness of the Discrete Solution \newline
	The discrete probelm \eqref{eq:discrete_minimization} always has a unique solution.
	\begin{proof}
		This follows directly from the previous lemma, as $B_1 + B_2$ is a positive definite matrix. The proof of this result can be found in standard literature, such as \cite{ulbrich2012nichtlineare}.
	\end{proof}
\end{corollary}

\chapter{Error Estimates}
The goal of this chapter is to analyze how good the solution to the discrete problem \eqref{eq:discrete_minimization} approximates the solution of the continuous one \eqref{eq:minimization_problem}. We will analyze the convergence order as a function of the mesh width $h$ with respect to several norms, and show that these convergence orders are optimal, as they are of the same order as the interpolation error for the same equidistant nodes, c.f. \cite{brenner2002mathematical}. 

For the entirety of this chapter, let $u \in H^2(\Omega)$ be the unique solution of \eqref{eq:variational_inequality} for a fixed $f \in L^2(\Omega)$. Furthermore, let $u_h \in V_h$ be the solution of \eqref{eq:discrete_variational_inequality} for the same $f$. Note that this implies that all constants mentioned in the chapter depend on the choice of $f$. Specifically, they all depend on the $L^2$-norm of $f$.

\section{The Ritz Projection}
In order to analyze the discretization error, it comes naturally to mind to estimate it by the least possible distance to a finite element function, by means of the Ritz Projection, and using the triangle inequality, subsequently to the solution of the discrete problem.

\subsection{Definition}

\begin{definition} The Ritz Projection \newline
	The Ritz Projection $R_h : H^1(\Omega) \rightarrow V_h$ is the orthogonal projection from $H^1(\Omega)$ to the subspace $V_h$, as uniquely defined by:
	\begin{equation}\label{eq:Ritz_Projection}
	(u-R_h(u), v_h)_{H_1} = 0 \quad \forall v_h \in V_h
	\end{equation}
	Because of the orthogonality, $R_h(u)$ is the solution to the following minimization problem:
	\begin{equation}\label{eq:ritz_minimization}
	R_h(u) = \argmin_{\substack{v_h \in V_h}} \norm{v_h - u}_{H_1}
	\end{equation}
\end{definition}
We will therefore derive an upper bound for the error between the Ritz Projection of the solution $u$ and the discrete solution $v_h$. To that avail, following the lines of \cite{2019christof}, we will firstly formulate the Ritz Projection in a way that links it to the solution of the Signorini Problem:

\begin{lemma} Ritz Projection of a Signorini Solution \newline
	We define $R_h(u)$ to be the unique solution $\tilde{u}_h$ of the following variational inequality:
	\begin{equation}\label{eq:ritz_variational}
	(\tilde{u}_h, v_h-\tilde{u}_h)_{H^1(\Omega)} \geq (f, v_h-\tilde{u}_h)_{L^2(\Omega)} \quad \forall v_h \in \tilde{K}_h
	\end{equation}
	where $\tilde{K}_h$ is defined as follows:
	\begin{align*}
	\tilde{K}_h \coloneqq \{v_h \in V_h \,|\,  v_h(1) &\geq (R_h(u) - u)(1) \quad \text{and} \\
	 v_h(-1) &\geq (R_h(u) - u)(-1)\}
	\end{align*}
	\begin{proof}
		The variatonal inequality \eqref{eq:ritz_variational} admits a unique solution. This follows in exactly the same way as \thref{thm:ex_uniq_discrete_sol}, as this problem has the same matrix representation, only with different boundary conditions. This variational inequality is equivalent to the same. To show that this is indeed $R_h(u)$, note that $R_h(u) \in \tilde{K}_h$, as $u$ is nonnegative in $\partial\Omega$. It therefore suffices to show that $R_h(u)$ fulfills the inequality. Let $v_h \in \tilde{K}_h$, then:
		\begin{align*}
		(R_h(u), v_h - R_h(u))_{H^1(\Omega)} \overset{\eqref{eq:Ritz_Projection}}&{=} (u, v_h - R_h(u))_{H^1(\Omega)} \\
		&= (u, v_h - R_h(u) + u - u)_{H^1(\Omega)} \\
		\overset{\eqref{eq:variational_inequality}}&{\geq} (f, v_h - R_h(u) + u - u)_{L^2(\Omega)} \\
		&= (f, v_h - R_h(u))_{L^2(\Omega)}
		\end{align*}
	\end{proof}
\end{lemma}


\subsection{Supercloseness}

We will use this representation to conclude the following supercloseness result. We will see that the error between the Ritz Projection $R_h(u)$ and the discrete solution $u_h$ is of the same order (in the case of the $L^\infty$-norm), or of higher order (in the case of the $H^1$-norm) than the order of the total error, so we may safely add it in with the triangle inequality.

\begin{lemma} Supercloseness Lemma \newline
	The $H^1$-error between $u_h$ and the Ritz Projection of $u$ is bounded as follows:
	\begin{align} \label{eq:supercloseness_lemma}
	\begin{split}
	&\norm{u_h - R_h(u)}_{H^1(\Omega)} \\ &\leq \inf \left\{
	\norm{w_h}_{H^1(\Omega)} \,\big|\, w_h \in V_h, \quad (R_h(u) - u)(\pm 1) \leq w_h(\pm 1) \leq R_h(u)(\pm 1)
	\right\}
	\end{split}
	\end{align}
	\begin{proof}
		Let $w_h$ be in the feasible set of the right hand side (which is necessarily non-empty, since it contains $R_h(u)$). As $R_h(u)$ is the solution of \eqref{eq:ritz_variational}, we have:
		\begin{equation*}
		(R_h(u), v_h - R_h(u))_{H^1(\Omega)} \geq (f, v_h - R_h(u))_{L^2(\Omega)}
		\end{equation*}
		for all $v_h \in \tilde{K}_h$, in particular for all $v_h \in \tilde{K}_h$ with $v_h(\pm 1) \geq w_h(\pm 1)$. In particular, we may choose $v_h \coloneqq u_h + w_h$, and obtain:
		\begin{equation*}
		(R_h(u), R_h(u)-u_h)_{H^1(\Omega)} \leq (R_h(u),w_h)_{H^1(\Omega)} + (f,R_h(u)-u_h-w_h)_{L^2(\Omega)}
		\end{equation*}
		On the other hand, we also know that $R_h(u)-w_h \geq R_h(u) - R_h(u) = 0$ on $\partial\Omega = \{\pm 1\}$. Thus, we can also choose $v_h \coloneqq R_h(u) - w_h$ as a test function and obtain:
		\begin{equation*}
		(u_h, u_h-R_h(u))_{H^1(\Omega)} \leq (u_h,-w_h)_{H^1(\Omega)} + (f,u_h + w_h - R_h(u))_{L^2(\Omega)}
		\end{equation*}		
		We may now add both inequalities to obtain:
		\begin{equation*}
		\norm{R_h(u)-u_h}^2_{H^1(\Omega)} \leq (R_h(u)-u_h, w_h)_{H^1(\Omega)}
		\end{equation*}
		As $w_h$ was an arbitrary function from the feasible set of which the infimum is taken, the claim follows.
	\end{proof}
\end{lemma}

\begin{corollary} Supercloseness \newline
	Under the same conditions of the previous lemma, the following bound also holds:
	\begin{equation}\label{eq:supercloseness}
	\norm{u_h - R_h(u)}_{H^1(\Omega)} \leq \frac{7}{3} \max \left(\abs{(u-R_h(u))(-1)}, \abs{(u-R_h(u))(1)}\right)
	\end{equation}
	\begin{proof}
		Recall the bound from \eqref{eq:supercloseness_lemma}. Let $w$ be defined as the linear interpolate of $y_0 \coloneqq (R_h(u) - u)(-1)$ and $y_1 \coloneqq (R_h(u) - u)(1)$, which reads:
		\begin{equation}
		w(x) = \frac{y_0(1-x)+y_1(1+x)}{2}
		\end{equation}
		This function trivially fulfills the boundary conditions from \eqref{eq:supercloseness_lemma}, and therefore its norm bounds the infimum from above.
		Let us therefore compute the norm explicitly:
		\begin{align*}
		\norm{w}_{H^1(\Omega)}^2 &= \int_{-1}^1{\abs{w}^2 + \abs{w'}^2} \dif x\\
		&= \frac{1}{6}(7y_0^2 + 7 y_1^2 - 2y_0y_1)  \leq \frac{7}{6} (y_0^2 + y_1^2)
		\end{align*}
		With this we follow the claim:
		\begin{align*}
		\norm{w}_{H^1(\Omega)} \leq \frac{7}{6} (\abs{y_0} + \abs{y_1}) \leq \frac{7}{3} \max(\abs{y_0}, \abs{y_1})
		\end{align*}
	\end{proof}
\end{corollary}

\subsection{$L^{\infty}$ and $H^1$ Error Estimates}

Firstly we will analyze the $L^{\infty}$ error, which we will subsequently also use to estimate the $H^1$ error.

\begin{definition} Lagrange-Interpolation \newline
	For a mesh width $h$ and corresponding nodes $-1=x_0<x_1<\dots<x_N = 1$, where $h = x_{i+1} - x_i$, the Lagrange-Interpolation Operator maps a continuous function to the linear spline interpolating at these nodes. It is uniquely characterized by the value at the nodes $x_i$:
	\begin{align}
	\begin{split}
	I_h : C(\bar{\Omega}) &\rightarrow V_h \\
	I_h(u)(x_i) &= u(x_i)
	\end{split}
	\end{align}
\end{definition}

\begin{lemma} Bound for the $L^\infty$-error of the Ritz Projection \newline
	There exists a $C \in \mathbb{R}_{>0}$ such that the following holds:
	\begin{equation} \label{eq:linfty_error_lemma}
		\norm{u -R_h(u)}_{L^\infty} \leq Ch^{\sfrac{3}{2}}\norm{u}_{H^2}
	\end{equation}
	\begin{proof}
		The main idea is to introduce the Lagrange Interpolate through Minkowski and then to use the Inverse Estimate for Finite Elements, as can be found in a very general setting in \cite{brenner2002mathematical}:
		\begin{align} \label{eq:lemma6part1}
		\norm{u -R_h(u)}_{L^\infty} \overset{\text{Minkowski}}&{\leq} \hspace{0.8em} \norm{u -I_h(u)}_{L^\infty} +\norm{I_h(u) -R_h(u)}_{L^\infty} \nonumber \\
		\overset{\text{\cite[Thm 4.5.11]{brenner2002mathematical}}}&{\leq}
		\norm{u -I_h(u)}_{L^\infty} + C_1 h^{-\sfrac{1}{2}}\norm{I_h(u) -R_h(u)}_{L^2} \\
		\overset{\text{Minkowski}}&{\leq} \hspace{0.8em} \norm{u -I_h(u)}_{L^\infty} + C_1 h^{-\sfrac{1}{2}}\norm{I_h(u) -u}_{L^2} + C_1 h^{-\sfrac{1}{2}}\norm{u -R_h(u)}_{L^2} \nonumber
		\end{align}
		We now need a suitable estimate for these three parts:
		\begin{align}\label{eq:lemma6part2}
		\begin{split}
			\norm{u -I_h(u)}_{L^\infty} \overset{\text{\cite[Thm 4.4.20]{brenner2002mathematical}}}&{\leq} C_2 h^{\sfrac{3}{2}}\norm{u}_{H^2} \\
			\norm{u -I_h(u)}_{L^2 \phantom{2}} \overset{\text{\cite[Thm 4.4.20]{brenner2002mathematical}}}&{\leq} C_3 h^2\norm{u}_{H^2} \\
			\norm{u -R_h(u)}_{L^2 \phantom{2}} \overset{\text{\cite[Lemma 3.2]{2019christof}}}&{\leq} C_4 h^2\norm{u}_{H^2} 
		\end{split}
		\end{align}
		Combining \eqref{eq:lemma6part1} and \eqref{eq:lemma6part2} we obtain the desired result:
		\begin{align*}
		\norm{u -R_h(u)}_{L^\infty} &\leq 	C_2 h^{\sfrac{3}{2}}\norm{u}_{H^2} + C_1 C_3 h^{-\sfrac{1}{2}}h^2\norm{u}_{H^2} + C_1 C_4 h^{-\sfrac{1}{2}}h^2\norm{u}_{H^2} \\
		&\leq Ch^{\sfrac{3}{2}}\norm{u}_{H^2}
		\end{align*}
	\end{proof}
\end{lemma}

\begin{theorem} $L^\infty$ Error Estimate of the Discrete Solution \newline
	There exists a $C \in \mathbb{R}_{>0}$ such that the following holds:
	\begin{equation}
		\norm{u-u_h}_{L^\infty(\Omega)} \leq C h^{\sfrac{3}{2}}
	\end{equation}
\begin{proof}
	As one might imagine from the previous results, we will introduce the Ritz projection through the triangle inequality, and then estimate the error from it to both the continuous and the discrete result:
	\begin{align*}
	\norm{u-u_h}_{L^\infty(\Omega)} &\leq \norm{u-R_h(u)}_{L^\infty(\Omega)} + \norm{R_h(u) - u_h}_{L^\infty(\Omega)} \\
	\overset{\eqref{eq:supercloseness} \eqref{eq:linfty_error_lemma}}&{\leq} 
	C_1 h^{\sfrac{3}{2}}\norm{u}_{H^2(\Omega)} + C_2 \max \left(\abs{(u-R_h(u))(-1)}, \abs{(u-R_h(u))(1)}\right) \\
	&\leq C_1 h^{\sfrac{3}{2}}\norm{u}_{H^2(\Omega)} + C_2 \norm{u-R_h(u)}_{L^\infty(\Omega)} \\
	\overset{\eqref{eq:linfty_error_lemma}}&{\leq} C_1 h^{\sfrac{3}{2}}\norm{u}_{H^2(\Omega)}  + C_3 h^{\sfrac{3}{2}} \norm{u}_{H^2(\Omega)}
	\\
	\overset{\eqref{eq:H2_norm_bound_by_f}}&{\leq} C h^{\sfrac{3}{2}} \norm{f}_{L^2(\Omega)}
	\end{align*}
	Since $f$ was fixed, the result follows.
\end{proof}
\end{theorem}

The $H^1$ error follows directly from the above bound and the interpolation error:

\begin{theorem}$H^1$ Error Estimate of the Discrete Solution \newline
	There exists a $C \in \mathbb{R}_{>0}$ such that the following holds:
	\begin{equation} \label{eq:H1_error}
		\norm{u-u_h}_{H^1(\Omega)} \leq C h
	\end{equation}
	\begin{proof}
		\begin{align*}
		\norm{u-u_h}_{H^1(\Omega)} &\leq \norm{u-R_h(u)}_{H^1(\Omega)} + \norm{R_h(u) - u_h}_{H^1(\Omega)} \\
		\overset{\eqref{eq:supercloseness}}&{\leq} \norm{u-R_h(u)}_{H^1(\Omega)} + C_1 \max \left(\abs{(u-R_h(u))(-1)}, \abs{(u-R_h(u))(1)}\right) \\
		&\leq \norm{u-R_h(u)}_{H^1(\Omega)} + C_1 \norm{u-R_h(u)}_{L^\infty(\Omega)} \\
		\overset{\eqref{eq:ritz_minimization} \eqref{eq:linfty_error_lemma}}&{\leq} \norm{I_h(u) - R_h(u)}_{H^1(\Omega)} + C_1 h^{\sfrac{3}{2}} \norm{u}_{H^2(\Omega)} \\
		\overset{\text{\cite[Thm 4.4.20]{brenner2002mathematical}}}&{\leq} C_2 h + C_1 h^{\sfrac{3}{2}}\norm{u}_{H^2(\Omega)} \\
		\overset{\eqref{eq:H2_norm_bound_by_f}}&{\leq} C h \norm{f}_{L^2(\Omega)}
		\end{align*}
		Since $f$ was fixed, the result follows.
	\end{proof}
\end{theorem}

\section{$L^2$ Error Through Duality}

The estimation of the $L^2$ error is more involved. We will follow the lines of \cite{2019christof}, which in turn follows \cite[Section 7]{mosco2006}, who proposed this approach for a similar obstacle problem. The key idea is to formulate two so-called dual variational inequalities, one for the positive component $\max(0, u-u_h)$ and one for the negative component $\min(0, u-u_h)$ of the error. From these we will derive useful inequalities.

\begin{definition} The Dual Problems \newline
	Let $L \coloneqq \left\{ v \in H^1(\Omega) \,|\, v(\pm 1) \geq 0 \,\,\text{if}\,\, u(\pm 1) = 0 \right\}$.Then we define the dual problem as follows:
	\begin{align} 
		z \in L,\quad (z,v-z)_{H^1(\Omega)} \geq (-\max(0,u-u_h),v-z)_{L^2(\Omega)} \quad \forall v \in L \label{eq:dual_variational_inequality} \\
		z \in L,\quad (z,v-z)_{H^1(\Omega)} \geq (-\min(0,u-u_h),v-z)_{L^2(\Omega)} \quad \forall v \in L
	\end{align}
\end{definition}

Firstly, we will need some tools to relate a function to its positive and negative components:

\begin{lemma} Stampacchia's Lemma \newline
	Let $u \in H^1(\Omega)$. Then the following holds:
	\begin{align}
	\begin{split} \label{eq:stampacchia_H1}
	\max(0,u) &\in H^1(\Omega) \\
	\min(0,u) &\in H^1(\Omega)
	\end{split} \\
	\nonumber \\ 
	\begin{split} \label{eq:stampacchia_derivative}
	\left(\max(0,u)\right)' = \max(0,u') &= \begin{cases}
	u' &\text{in}\,\, \{u > 0\}, \\
	0  &\text{in}\,\, \{u \leq 0\}
	\end{cases} \\
	\left(\min(0,u)\right)' = \min(0,u') &= \begin{cases}
	u' &\text{in}\,\, \{u < 0\}, \\
	0  &\text{in}\,\, \{u \geq 0\}
	\end{cases}
	\end{split}
	\end{align}
	\begin{proof}
		There are many theorems referred to as ``Stampacchia's Lemma'' in the literature. This is the one found in \cite[Theorem A.1]{KinderlehrerStampacchia} and \cite[Theorem 5.8.2]{Attouch09}.
	\end{proof}
\end{lemma}


For simplicity, for the remainder of this section, we will consider only the positive component, as it is completely analogous to the negative one, and all claims regarding the one are also trivially true for the other.

\begin{lemma}Existence of a Unique Dual Solution \newline 
	The dual problem \eqref{eq:dual_variational_inequality} admits a unique solution $z \in H^2(\Omega)$ for all $h > 0$. Furthermore, for this solution it holds that:
	\begin{equation} \label{eq:dual_solution_nonpositive}
	z \leq 0 \quad \text{in} \quad \bar{\Omega}
	\end{equation}
	\begin{proof}
		Fixing a $h>0$, the existence and uniqueness of a solution follows from \thref{thm:unique_solution_signorini}, taking into account that $-\max(0,u-u_h) \in L^2(\Omega)$, and that $L$ always fulfills the generalized boundary conditions, independently of $u$.
		
		For the proof of the inequality, let $z$ be the unique solution, note that $v \coloneqq \min(0,z) \in L$ and test with it in \eqref{eq:dual_variational_inequality}, taking into account that $v - \min(0,v) = \max(0,v)$:
		\begin{align*}
			-(z,\max(0,z))_{H^1(\Omega)} &\geq (\max(0,u-u_h), \max(0,z))_{L^2(\Omega)} 
		\end{align*}
		Multiplying by $-1$, and applying Stampacchia's Lemma, we obtain:
		\begin{align*}
		\norm{\max(0,z)}_{H^1(\Omega)}^2 \overset{\eqref{eq:stampacchia_derivative}}{=} (z,\max(0,z))_{H^1(\Omega)} \leq - (\max(0,u-u_h), \max(0,z))_{L^2(\Omega)} \leq 0
		\end{align*}
		This implies the following:
		\begin{equation*}
		\norm{\max(0,z)}_{H^1(\Omega)}^2 \overset{\eqref{eq:sobolev1_norm}}{=}\norm{\max(0,z)}_{L^2(\Omega)}^2 + \norm{\left(\max(0,z)\right)'}_{L^2(\Omega)}^2 = 0
		\end{equation*}
		We see that the positive part of $z$ disappears $\mathcal{L}$-a.e., which implies the claim by taking the continuous representative of $z$ provided by the Sobolev embedding \eqref{eq:sobolev1_embedding}.
	\end{proof}
\end{lemma}


We will now consider a solution $z \in H^2(\Omega)$ of \eqref{eq:dual_variational_inequality}, and test appropriate functions containing it in all three variational inequalities to obtain the following inequalities relating the $L^2$ to the $H^1$ error:

\begin{lemma}
	Let $z \in H^2(\Omega)$ be a solution of \eqref{eq:dual_variational_inequality}, with corresponding $u$, $u_h$ and $f$. Then the following inequalities hold:
	\begin{align}
		\norm{\max(0,u-u_h)}_{L^2(\Omega)}^2 &\leq (z,u_h-u)_{H^1(\Omega)} \\
		(f, I_h(z))_{L^2} &\leq (u, I_h(z))_{H^1(\Omega)} \\
		(f, -I_h(z))_{L^2} &\leq (u_h, -I_h(z))_{H^1(\Omega)} \\
		\norm{\max(0,u-u_h)}_{L^2(\Omega)}^2 &\leq (z-I_h(z), u_h - u)_{H^1(\Omega)} \label{eq:scalar_product_ineq_L2}
	\end{align}
	\begin{proof}
		For the first inequality, we test \eqref{eq:dual_variational_inequality} against $z+u_h-u$. This function is in $L$. This only needs to be proven for the case where there are boundary conditions to be enforced, i.e. $u(1) = 0$ or $u(-1) = 0$. W.l.o.g. let $u(1) = 0$: 
		\begin{equation*}
		(z + u_h - u)(1) \overset{u(1) = 0}{=} (z + u_h)(1) \overset{z \in L,\, \eqref{eq:dual_solution_nonpositive}}{=} (u_h)(1) \overset{u_h \in K}{\geq} 0
		\end{equation*}
		The claim follows:
		\begin{align*}
		(z, u_h-u)_{H^1(\Omega)} &\geq (-\max(0,u-u_h), u_h-u)_{L^2(\Omega)} \\
		&= (\max(0,u_h-u), u-u_h)_{L^2(\Omega)} \\
		&= \norm{\max(0,u-u_h)}_{L^2(\Omega)}^2
		\end{align*}
		
		For the second one, we observe that as $u$ is a solution of \eqref{eq:variational_inequality}, it holds that $u(\pm 1) \geq 0$. Furthermore, following \eqref{eq:dual_solution_nonpositive}, we have $z(\pm 1) \leq 0$, and as the interpolate is equal at the nodes, also $I_h(z)(\pm 1) \leq 0$. If we choose $s \in \mathbb{R}_{>0}$ sufficiently small, then $u + s I_h(z) \in K$, and therefore admissible in \eqref{eq:variational_inequality}. We thus obtain:
		\begin{equation*}
		(u, sI_h(z))_{H^1(\Omega)} \geq (f,sI_h(z))_{L^2(\Omega)}
		\end{equation*}
		Dividing both sides by the positive constant $s$, the claim follows.
		
		For the third inequality, following a similar argument as before, we conclude that $u_h -I_h(z)$ is admissible for \eqref{eq:discrete_variational_inequality}. The claim follows directly.
		
		The last inequality follows by simply adding the three previous ones.
	\end{proof}
\end{lemma}

\begin{corollary}\thlabel{thm:L2_error_lemma} $L^2$ Error for the Positive Component \newline
	Let $u \in H^2(\Omega)$ be a solution of \eqref{eq:variational_inequality} for some $f \in L^2(\Omega)$ and $u_h \in V_h$ the solution of \eqref{eq:discrete_variational_inequality} for the same $f$. Then it holds:
	\begin{equation} \label{eq:L2_error_lemma}
	\norm{\max(0,u-u_h)}_{L^2(\Omega)} \leq Ch^2
	\end{equation}
	\begin{proof}
		\begin{align*}
		\norm{\max(0,u-u_h)}_{L^2(\Omega)}^2 \overset{\eqref{eq:scalar_product_ineq_L2}}&{\leq} (z-I_h(z),u_h-u)_{H^1(\Omega)} \\
		\overset{\begin{subarray}{c} \text{Cauchy-}\\ \text{Schwarz}
			\end{subarray}}&{\leq} \norm{z-I_h(z)}_{H^1(\Omega)} \norm{u_h - u}_{H^1(\Omega)} \\
			\overset{\substack{\text{\cite[Thm 4.4.20]{brenner2002mathematical}} \\ \eqref{eq:H1_error}}}&{\leq} C_1h \norm{z}_{H^2(\Omega)} C_2h 
			\overset{\eqref{eq:H2_norm_bound_by_f}}{\leq} Ch^2 \norm{\max(0,u-u_h)}_{L^2(\Omega)}
		\end{align*}
		Dividing both sides by $\norm{\max(0,u-u_h)}_{L^2(\Omega)}$ yields the claim.
	\end{proof}
\end{corollary}

\begin{theorem}
	 $L^2$ Error for the Whole Solution \newline
	Let $u \in H^2(\Omega)$ be a solution of \eqref{eq:variational_inequality} for some $f \in L^2(\Omega)$ and $u_h \in V_h$ the solution of \eqref{eq:discrete_variational_inequality} for the same $f$. Then it holds:
	\begin{equation}
	\norm{u-u_h}_{L^2(\Omega)} \leq Ch^2
	\end{equation}
	\begin{proof}
		This follows from the above \thref{thm:L2_error_lemma}, the analogous result for the negative component, and the following:
		\begin{align*}
		\norm{u-u_h}_{L^2(\Omega)} &\leq  \norm{u-u_h}_{L^2(\Omega \cap \{ u-u_h \geq 0\})} + \norm{u-u_h}_{L^2(\Omega \cap \{ u-u_h < 0\})} \\
		&= \norm{\max(0,u-u_h)}_{L^2(\Omega \cap \{ u-u_h \geq 0\})} + \norm{\min(0,u-u_h)}_{L^2(\Omega \cap \{ u-u_h < 0\})} \\
		&= \norm{\max(0,u-u_h)}_{L^2(\Omega)} + \norm{\min(0,u-u_h)}_{L^2(\Omega)} \\
		\overset{\eqref{eq:L2_error_lemma}}&{\leq}  Ch^2 
		\end{align*}
	\end{proof}
\end{theorem}


\chapter{Numerical Tests}
We will use the popular numerics tool Matlab \cite{MATLAB2020} for the implementation, as it provides builtin tools to solve the subproblems that arise in the method which we do not handle in this paper, such as quadratic programming and quadrature. The actual code used for the experiments below can be found under \cite{Loewenberg2020}.

For the entirety of this chapter, let $u \in H^2(\Omega)$ be the unique solution of \eqref{eq:variational_inequality} for a given $f \in L^2(\Omega)$. Furthermore, let $u_h \in V_h$ be the solution of \eqref{eq:discrete_variational_inequality} for the same $f$.

\section{Methods}
\subsection{Minimization}
We will describe the techniques used to implement the discrete minimization problem \eqref{eq:discrete_minimization} and test its accuracy.

The mass and stiffness matrices are distinctly sparse, so we use the \verb|spdiags| function to implement them in an efficient way. 
The implementation of the vector $\mathbf{f}$ requires integrating an unknown function, so the Gauss-Kronrod type quadrature builtin \verb|quadgk| is used. If this is used naively over the entire domain, the algorithm is not able to achieve the high precision required. This is of course due to the very small support of the basis functions $\varphi_i^h$, so a sort of additive quadrature formula is achieved by quadrating only over the support of the corresponding basis function:
\begin{equation}
\mathbf{f}_i = \int_{-1}^1 f\varphi_i^h\dif x = \int_{\max\{-1, x_{i-1}\}}^{\min\{1, x_{i+1}\}} f\varphi_i^h\dif x
\end{equation} 
The finite-dimensional minimization problem can then be easily solved via \verb|quadprog| to a high precision.

\subsection{Accuracy Test}

In this paper we have analyzed the convergence order of this method in three norms, so we will test our predictions in all three. Approximating every norm presents distinct challenges. Here we will distinguish between testing towards an analytical solution and testing towards a much finer, but still approximate solution.

In the first case we analye $\norm{u-u_h}_*$. The $L^\infty$ norm is approximated as follows:
\begin{align}
\norm{u-u_h}_{L^\infty(\Omega)} &\approx \\ &\max \left\{\abs{u(x) - u_h(x)} \,|\, x \in \{-1 = x_0,\dots, x_n = 1, x_{i+1} - x_i = \sfrac{h}{1000}\} \right\} \nonumber 
\end{align}
It is important that the mesh is finer than the one used for the finite-dimensional problem, since obviously the most divergence will be found between the nodes.

The $L^2$ and $H^1$ norms are approximated also using \verb|quadgk|, here also only quadrating individual intervals between nodes:
\begin{align}
\begin{split}
	\norm{u-u_h}_{L^2(\Omega)} &= \sqrt{\sum_{i=0}^{N-1} \norm{u-u_h}_{L^2(x_i, x_{i+1})}^2 } \\
	\norm{u-u_h}_{H^1(\Omega)} &= \sqrt{\sum_{i=0}^{N-1} \norm{u-u_h}_{L^2(x_i, x_{i+1})}^2 + \sum_{i=0}^{N-1} \norm{u'-u_h'}_{L^2(x_i, x_{i+1})}^2} 
\end{split}
\end{align}
 Here the accuracy problem is not the support, but the large number of discontinuities at the nodes. This renders the quadrature formulas less accurate.

In the second case, we can actually calculate the norms exactly, as we are dealing with infinite dimensional spaces. Let $\tilde{h}$ be the mesh width of the reference solution. The key idea is to embed the current approximate solution $u_h \in V_h$, which is of course just a vector $\mathbf{v}^h \in \mathbb{R}^{\mathrm{dim}\, V_h}$ into $V_{\tilde{h}}$. This can be done if the chosen mesh widths are all a negative power of two, and therefore the nodes in the lesser dimensional space are also nodes in the higher one. The nodes in between can be calculated by linear interpolation as follows:
\begin{align*}
	\mathbf{v}^{\sfrac{h}{2}} = \left(\mathbf{v}^h_0, \frac{\mathbf{v}^h_0 + \mathbf{v}^h_1}{2} , \mathbf{v}^h_1, \dots, \mathbf{v}^h_{N-1}, \frac{\mathbf{v}^h_{N-1} + \mathbf{v}^h_N}{2}, \mathbf{v}^h_N \right)
\end{align*}

In Matlab \verb|interp1| is used, which does several interpolation steps, avoiding the need for recursion. If we denote the reference solution vector as $\mathbf{v}^{\tilde{h}}$ and the interpolation of a lower dimensional one into $V_{\tilde{h}}$ as $\tilde{\mathbf{v}}^h$, then the norms can be exactly calculated as follows:
\begin{align}
	\norm{u_{\tilde{h}} - u_h}_{L^\infty(\Omega)} &= \max_{i \in \{0,\dots,\mathrm{dim} \, V_{\tilde{h}}\}} \abs{\mathbf{v}^{\tilde{h}}_i - \tilde{\mathbf{v}}^h_i} \\
	\norm{u_{\tilde{h}} - u_h}_{L^2(\Omega)} &=  \sqrt{(\mathbf{v}^{\tilde{h}} - \tilde{\mathbf{v}}^h)^T B_1 (\mathbf{v}^{\tilde{h}} - \tilde{\mathbf{v}}^h)}\\
	\norm{u_{\tilde{h}} - u_h}_{H^1(\Omega)} &= \sqrt{(\mathbf{v}^{\tilde{h}} - \tilde{\mathbf{v}}^h)^T (B_1+B_2) (\mathbf{v}^{\tilde{h}} - \tilde{\mathbf{v}}^h)}
\end{align}
The first result follows simply from the piecewise linearity of the involved functions. The second and third one follow from \eqref{eq:matrix_formulation}, since $B_1$ and $B_1 + B_2$ are transformation matrices for the $L^2$ and $H^1$ scalar products, respectively.

Once we have computed the norms, we may consider the Experimental Orders of Convergence (EOCs), which are defined for a sequence of mesh widths $(h_i)_{i=1}^N$, in our case $h_i = \sfrac{1}{2^i}$, and every norm as the following quantities:

\begin{equation}
	(\mathrm{EOC})_{h_k,\,\norm{\cdot}_*} \coloneqq \frac{\log \lVert u-u_{h_k}\rVert_* - \lVert\log u-u_{h_{k-1}}\rVert_* }{\log h_k - \log h_{k-1}}
\end{equation}

\section{Results}
Firstly, we will try to analyze the method using a right hand side $f$ which allows for an analytical classical solution, and can therefore be used as a reference in our numerical experiments. We will therefore consider the following instance of the Signorini Problem:
\begin{align*}
-u'' + u &= \sin \quad \textnormal{in} \quad \Omega \\ 
\partial_n u \geq 0,\quad u &\geq 0,\quad u\partial_nu = 0 \quad\textnormal{on}\quad \partial \Omega 
\end{align*}
It is easy to see that a solution to this problem has the following form:
\begin{equation*}
	u(x) = c_1 e^x + c_2 e^{-x} + \frac{\sin(x)}{2}
\end{equation*}
For some $c_1,c_2 \in \mathbb{R}$. Enforcing the boundary conditions leads to the following coefficients:
\begin{align*}
c_1 = - \frac{e^3\cos(1) - e^1\sin(1)}{2(e^4 + 1)}, \quad c_2 = \frac{e^1\cos(1) + e^3\sin(1)}{2(e^4 + 1)}
\end{align*}

\pagebreak
\begin{figure}[h!]
	\centerfloat
	\begin{tabular}{c@{\hskip -1em}c}
	\input{sin_sol.tex} & \input{sin_sol_derivative.tex}	
	\end{tabular}
	\caption[Solution to $f = \sin$]{To the left, the right hand side of the problem $f$, the exact solution $u$, and a discrete solution $u_h$. To the right, the derivatives of the continuous and discrete solutions (with a different mesh width for illustrative purposes).}
\end{figure}

\begin{figure}[h!]
	\centering
		\bgroup
		\def\arraystretch{1.3}
		\begin{tabular}{cccc}
			\hline
			$h$ & $L^\infty(\Omega)$ & $H^1(\Omega)$ & $L^2(\Omega)$ \\
			\hline
			$\sfrac{1}{2}$      	&	\num{1.9989e-02}	&	\num{6.0676e-02}	&	\num{1.2335e-02}\\
			$\sfrac{1}{2^2}$	&	\num{5.7357e-03}	&	\num{3.5344e-02}	&	\num{3.1109e-03}	\\
			$\sfrac{1}{2^3}$	&	\num{1.5349e-03}	&	\num{1.9586e-02}	&	\num{7.7933e-04}	\\
			$\sfrac{1}{2^4}$	&	\num{3.9704e-04}	&	\num{1.0339e-02}	&	\num{1.9493e-04}	\\
			$\sfrac{1}{2^5}$	&	\num{1.0097e-04}	&	\num{5.3138e-03}	&	\num{4.8739e-05}	\\
			$\sfrac{1}{2^6}$	&	\num{2.5460e-05}	&	\num{2.6938e-03}	&	\num{1.2185e-05}	\\
			$\sfrac{1}{2^7}$	&	\num{6.3924e-06}	&	\num{1.3562e-03}	&	\num{3.0463e-06}	\\
			$\sfrac{1}{2^8}$	&	\num{1.6015e-06}	&	\num{6.8046e-04}	&	\num{7.6157e-07}	\\
			$\sfrac{1}{2^9}$	&	\num{4.0081e-07}	&	\num{3.4082e-04}	&	\num{1.9039e-07}	\\
			$\sfrac{1}{2^{10}}$	&	\num{1.0026e-07}	&	\num{1.7056e-04}	&	\num{4.7594e-08}	\\						
			\end{tabular}
			\egroup 
	\caption[Errors for  $f = \sin$]{Absolute errors between a finite element solution $u_h$ of mesh width $h$ and the exact solution $u$ for a right hand side $f = \sin$.}
\end{figure}
\pagebreak

\begin{figure}[h!] 
	\centering
	\bgroup
	\def\arraystretch{1.3}
	\begin{tabular}{cccc}
		\hline
		$h$ & $L^\infty(\Omega)$ & $H^1(\Omega)$ & $L^2(\Omega)$ \\
		\hline
		$\sfrac{1}{2}$      	&	-	&	-	&	- \\
		$\sfrac{1}{2^2}$	&	\num{1.8012}	&	\num{0.7797}	&	\num{1.9873} \\
		$\sfrac{1}{2^3}$	&	\num{1.9018}	&	\num{0.8516}	&	\num{1.9970} \\
		$\sfrac{1}{2^4}$	&	\num{1.9508}	&	\num{0.9216}	&	\num{1.9993} \\
		$\sfrac{1}{2^5}$	&	\num{1.9753}	&	\num{0.9603}	&	\num{1.9998} \\
		$\sfrac{1}{2^6}$	&	\num{1.9876}	&	\num{0.9801}	&	\num{2.0000} \\
		$\sfrac{1}{2^7}$	&	\num{1.9938}	&	\num{0.9900}	&	\num{2.0000} \\
		$\sfrac{1}{2^8}$	&	\num{1.9969}	&	\num{0.9950}	&	\num{2.0000} \\
		$\sfrac{1}{2^9}$	&	\num{1.9985}	&	\num{0.9975}	&	\num{2.0000} \\
		$\sfrac{1}{2^{10}}$	&	\num{1.9992}	&	\num{0.9988}	&	\num{2.0001} \\
		\hline
		regr. & $1$ & $1$ & $1$ \\
		\hline										
		theo. & $1.5$ & $1$ & $2$
	\end{tabular}
	\egroup
	\caption[EOCs for  $f = \sin$]{Experimental orders of convergence (EOCs) for a sequence of finer finite element approximations $u_h$ of mesh width $h$ to the exact solution $u$ for a right hand side $f = \sin$.}
	\label{fig:eocs_sin}
\end{figure}

The table shown in Figure \ref{fig:eocs_sin} shows surprising results. While the convergence orders of the $H^1$-error and the $L^2$-error seem to fit the predictions nicely, the $L^\infty$-error converges too quickly. This apparently contradicts the prediction, that a convergence order of $\sfrac{3}{2}$ is optimal in the general case. 

The answer to this apparent contradiction lies exactly there, this is a special case since the right hand side $f = \sin$ lies also in $L^\infty(\Omega)$, and not only in $L^2(\Omega)$. Using the same argument we used in \eqref{eq:H2_regularity}, we can show that the solution consequently not only lies in $H^2(\Omega)$, but also in $W^{2,\infty}(\Omega)$, and its $H^2$-norm can also be bound by the $L^\infty$-norm of the right hand side. 

Consequently, the same arguments used to show the convergence order of $2$ for a function in $H^2(\Omega)$ w.r.t. the $L^2$-norm can be used to show that convergence order for a function in $W^{2,\infty}(\Omega)$ w.r.t. the $L^\infty$-norm, since we have the stronger bound for the $H^2$-norm of the solution.

We will therefore consider a right hand side $f \notin L^\infty(\Omega)$, or at least a function very close to one to avoid numerical anomalies. We achieve this with $f = \abs{x}^{\sfrac{1}{2} - \epsilon}$ (with $\epsilon = \sfrac{1}{1000}$). The Signorini problem does not have a simple analytical solution with this right hand side, however, so we will, as hinted in the previous section, construct a reference solution with a much finer mesh width $\tilde{h} = \sfrac{1}{2^{12}}$, and approximate the error of the coarser approximations using it.

\begin{figure}[h!]
	\centerfloat
	\begin{tabular}{c@{\hskip -1em}c}
		\input{L2_function_sol.tex} & \input{L2_function_sol_derivative.tex}	
	\end{tabular}
	\caption[Solution to $f = \abs{x}^{\sfrac{1}{2} - \epsilon}$]{To the left, the right hand side of the problem $f = \abs{x}^{0.499}$, the reference solution $u_{\tilde{h}}$, and a discrete solution $u_h$. To the right, the derivatives of the reference and discrete solutions (with a different mesh width for illustrative purposes).}
\end{figure}

\begin{figure}
	\centering
	\bgroup
	\def\arraystretch{1.3}
	\begin{tabular}{cccc}
		\hline
		$h$ & $L^\infty(\Omega)$ & $H^1(\Omega)$ & $L^2(\Omega)$ \\
		\hline
		$\sfrac{1}{2}$      &	\num{1.6742e-02}	&	\num{1.2787e-01}	&	\num{1.4965e-02} \\
		$\sfrac{1}{2^2}$	&	\num{9.1475e-03}	&	\num{8.5222e-02}	&	\num{5.7176e-03} \\
		$\sfrac{1}{2^3}$	&	\num{4.6159e-03}	&	\num{5.3941e-02}	&	\num{1.9150e-03} \\
		$\sfrac{1}{2^4}$	&	\num{2.0228e-03}	&	\num{3.2449e-02}	&	\num{5.9309e-04} \\
		$\sfrac{1}{2^5}$	&	\num{8.1881e-04}	&	\num{1.8801e-02}	&	\num{1.7485e-04} \\
		$\sfrac{1}{2^6}$	&	\num{3.1625e-04}	&	\num{1.0603e-02}	&	\num{4.9880e-05} \\
		$\sfrac{1}{2^7}$	&	\num{1.1865e-04}	&	\num{5.8610e-03}	&	\num{1.3901e-05} \\
		$\sfrac{1}{2^8}$	&	\num{4.3684e-05}	&	\num{3.1871e-03}	&	\num{3.8035e-06} \\
		$\sfrac{1}{2^9}$	&	\num{1.5682e-05}	&	\num{1.7037e-03}	&	\num{1.0208e-06} \\
		$\sfrac{1}{2^{10}}$	&	\num{5.6627e-06}	&	\num{8.8349e-04}	&	\num{2.6405e-07} 
	\end{tabular}
	\egroup 
	\caption[Errors for  $f = \abs{x}^{\sfrac{1}{2} - \epsilon}$]{Absolute errors between a finite element solution $u_h$ of mesh width $h$ and a reference solution $u_{\tilde{h}}$ for a right hand side $f = \abs{x}^{\sfrac{1}{2} - \epsilon}$.}
\end{figure}

\begin{figure}
	\centering
	\bgroup
	\def\arraystretch{1.3}
	\begin{tabular}{cccc}
		\hline
		$h$ & $L^\infty(\Omega)$ & $H^1(\Omega)$ & $L^2(\Omega)$ \\
		\hline
		$\sfrac{1}{2}$      &	-		&		-		&		- \\
		$\sfrac{1}{2^2}$	&	\num{0.8720}		&		\num{0.5854}		&		\num{1.3881} \\
		$\sfrac{1}{2^3}$	&	\num{0.9868}		&		\num{0.6598}		&		\num{1.5780} \\
		$\sfrac{1}{2^4}$	&	\num{1.1903}		&		\num{0.7332}		&		\num{1.6910} \\
		$\sfrac{1}{2^5}$	&	\num{1.3047}		&		\num{0.7873}		&		\num{1.7621} \\
		$\sfrac{1}{2^6}$	&	\num{1.3725}		&		\num{0.8263}		&		\num{1.8096} \\
		$\sfrac{1}{2^7}$	&	\num{1.4144}		&		\num{0.8553}		&		\num{1.8433} \\
		$\sfrac{1}{2^8}$	&	\num{1.4415}		&		\num{0.8789}		&		\num{1.8698} \\
		$\sfrac{1}{2^9}$	&	\num{1.4780}		&		\num{0.9036}		&		\num{1.8976} \\
		$\sfrac{1}{2^{10}}$	&	\num{1.4696}		&		\num{0.9474}		&		\num{1.9509} \\
		\hline
		regr. & $1$ & $1$ & $1$ \\
		\hline										
		theo. & $1.5$ & $1$ & $2$
	\end{tabular}
	\egroup
	\caption[EOCs for  $f = \abs{x}^{\sfrac{1}{2} - \epsilon}$]{Experimental orders of convergence (EOCs) for a sequence of finer finite element approximations $u_h$ of mesh width $h$ to the reference solution $u_{\tilde{h}}$ for a right hand side $f = \abs{x}^{\sfrac{1}{2} - \epsilon}$.}
	\label{fig:eocs_L2}
\end{figure}

The convergence orders for this new right hand side, as shown in Figure \ref{fig:eocs_L2} fit very well to the predictions in every norm.

\microtypesetup{protrusion=false}
\listoffigures{}


\printbibliography{}
\end{document}