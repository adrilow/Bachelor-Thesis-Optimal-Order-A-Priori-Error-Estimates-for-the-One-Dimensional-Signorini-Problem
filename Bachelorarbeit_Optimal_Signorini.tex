% !TeX spellcheck = en_US 
%\documentclass[headsepline,footsepline,footinclude=false,fontsize=11pt,paper=a4,listof=totoc,bibliography=totoc,BCOR=12mm,DIV=12]{scrbook} % two-sided
\documentclass[headsepline,footsepline,footinclude=false,oneside,fontsize=11pt,paper=a4,listof=totoc,bibliography=totoc]{scrbook} % one-sided

%\documentclass[english,a4paper,12pt,oneside]{scrbook}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{amsmath, amsthm, amssymb}
\allowdisplaybreaks
\usepackage{aligned-overset}
\usepackage{theoremref}
\usepackage{commath}
\usepackage{mathtools}
\usepackage{aligned-overset}
\usepackage{marvosym}
\usepackage{graphics}
\usepackage[%
backend=biber,
url=false,
style=numeric,
maxnames=4,
minnames=3,
maxbibnames=99,
sorting=none,
giveninits,
uniquename=init]{biblatex} % TODO: adapt citation style
\usepackage{microtype}
\usepackage{bera}
\usepackage{xcolor}


\bibliography{Bibliography}
\setkomafont{disposition}{\normalfont\bfseries} % use serif font for headings
\linespread{1.05} % adjust line spread for mathpazo font

\newenvironment{changemargin}[2]{%
	\begin{list}{}{%
			\setlength{\topsep}{0pt}%
			\setlength{\leftmargin}{#1}%
			\setlength{\rightmargin}{#2}%
			\setlength{\listparindent}{\parindent}%
			\setlength{\itemindent}{\parindent}%
			\setlength{\parsep}{\parskip}%
		}%
		\item[]}
	{\end{list}}

\makeatletter
\makeatletter
\newcommand{\mytag}[2]{%
	\text{#1}%
	\@bsphack
	\protected@write\@auxout{}%
	{\string\newlabel{#2}{{#1}{\thepage}}}%
	\@esphack
}
\makeatother

\makeatletter
\newcommand{\mytaghr}[2]{%
	\text{#1}%
	\@bsphack
	\begingroup
	\@onelevel@sanitize\@currentlabelname
	\edef\@currentlabelname{%
		\expandafter\strip@period\@currentlabelname\relax.\relax\@@@%
	}%
	\protected@write\@auxout{}{%
		\string\newlabel{#2}{%
			{#1}%
			{\thepage}%
			{\@currentlabelname}%
			{\@currentHref}{}%
		}%
	}%
	\endgroup
	\@esphack
}
\makeatother


\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

%\newtheorem{satz}{Satz}[chapter]
%\theoremstyle{definition} 
%\newtheorem{definition}[satz]{Definition} 
%\theoremstyle{definition} 
%\newtheorem{lemma}[satz]{Lemma} 
%\theoremstyle{definition} 
%\newtheorem{bemerkung}[satz]{Bemerkung}
%\theoremstyle{definition} 
%\newtheorem{korollar}[satz]{Korollar} 
%\theoremstyle{definition}
%\newtheorem{beispiel}[satz]{Beispiel} 
%\theoremstyle{definition} 
%\newtheorem{algorithmus}{Algorithmus} 
%\newenvironment{beweis}{\begin{proof}[Beweis]}{\end{proof}}

\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}
\newtheorem{corollary}{Corollary}

\setlength{\parindent}{0em}
\setlength{\parskip}{0.8em}

\begin{document}


% Titelseite
\pagestyle{empty}       % keine Seitennummer
  \parbox{1.5cm}{\resizebox*{110pt}{!}{\includegraphics{tum.pdf}}}\hspace{310pt}%
  \parbox{1.5cm}{\resizebox*{90pt}{!}{\includegraphics{FAK_MA_CMYK.pdf}}}%
\vspace*{1.5cm}
\begin{center}
{\huge \MakeUppercase{Department of Mathematics}} 
\\
\vspace*{5mm}
{\large \MakeUppercase{Technische Universität München} }
\\
\vspace*{2cm}
{\huge {\textbf{{Optimal-Order A-Priori Error Estimates} \\ for the
One-Dimensional\\ Signorini Problem}\par}}
\vspace*{2cm}
{\Large Bachelor's Thesis}\linebreak \\ 
{\Large by}\linebreak \\
{\Large Adrián Löwenberg Casas}\\
\vspace*{1.8cm}
{\large 
\begin{tabular}{ll}
Supervisor: & Prof. Dr. Boris Vexler\\
Advisor: & Dr. Constantin Christof\\
Submission Date: & 15th September 2020
\end{tabular}
}
\end{center}
\newpage    % Seitenwechsel

% Seite 2
\thispagestyle{empty}
\vspace*{0.70\textheight}
\noindent
I confirm that this bachelor’s thesis in mathematics is my own work and I have
documented all sources and material used.

\vspace{30mm}
Munich,\hspace{80mm} Adrián Löwenberg Casas
\enlargethispage{10\baselineskip}
\newpage

\addcontentsline{toc}{chapter}{Acknowledgments}
\thispagestyle{empty}

\vspace*{20mm}

\begin{center}
	{\usekomafont{section} Acknowledgments}
\end{center}

\vspace{10mm}
% Seitennummerierung römisch
\pagenumbering{roman}
% Kopfzeilen (automatisch erzeugt)
\pagestyle{headings}
[Text der Danksagung]
\newpage

% Seite 3
\addcontentsline{toc}{chapter}{Abstract}
\chapter*{}
\vspace*{-2.2cm}
\section*{Zusammenfassung auf Deutsch}
[Text der Zusammenfassung]
\section*{Zusammenfassung auf Englisch}
[Summary of the thesis]
%Seite 4
\newpage
\microtypesetup{protrusion=false}
\tableofcontents{}
\microtypesetup{protrusion=true} 


\pagenumbering{arabic}  % Nummerierung der Seiten in 'arabisch' % neues Kapitel mit Namen "Introduction"
\chapter{Einleitung}  \setcounter{page}{1}   % setzt Seitenzaehlung auf 1
Zitate aus B\"uchern werden so gemacht, siehe in \cite{schoenbucher03} oder \cite{marshall67}.

\chapter{Theory}

\begin{definition}The One Dimensional Signorini Problem \newline
	Let $\Omega \coloneqq (-1,1)$. We denote the boundary of $\Omega$ by $\partial\Omega = \{-1,1\}$ and the normal derivative of $u$ on $\partial\Omega$ by $\partial_nu$, i.e. $\partial_nu(-1) = -u'(-1)$ and $\partial_nu(1) = u'(1)$. Furthermore let $f \in L^2(-1,1)$ be the given forcing term. The One-Dimensional Signorini Problem is defined as follows:
	\begin{align}
	-u'' + u &= f \quad \textnormal{in} \quad \Omega \label{eq:signorini}\\ 
	\partial_n u \geq 0,\quad u &\geq 0,\quad u\partial_nu = 0 \quad\textnormal{on}\quad \partial \Omega \label{eq:boundary_signorini}
	\end{align}
\end{definition}

\begin{definition}One Dimensional Sobolev-Spaces \newline
	Let $p\in [1,\infty]$. For $k \in \mathbb{N}_0$, the Sobolev-Space $W^{k,p}(a,b)$ is defined as follows:
	\begin{align}
	\begin{split}
	W^{0,p}(a,b) &\coloneqq L^p(a,b) \\
	W^{k,p}(a,b) &\coloneqq \big\{ v \in W^{k-1,p}(a,b) \,|\, \exists w \in W^{k-1,p}(a,b), c\in\mathbb{R}. \\
	& \phantom{\coloneqq \big\{\,\,} v(t) =  c + \int_a^t w(\tau) \dif \tau \quad \text{f.a.a.} \,\, t\in (a,b)  \big\}
	\end{split}
	\end{align}
	For $v\in W^{k,p}$ the function $w$ in the definition is called the weak derivative of $v$, and in this context the notation $v' \coloneqq w$ is useful. Because of the Fundamental Theorem of Calculus, one can easily see that if $v$ has a classical derivative, then it coincides with the weak one. One can naturally expand this notation to the $k$th weak derivative $v^{(k)}$.
	
	$W^{k,p}(a,b)$ is a Banach Space with the following norm:
	\begin{equation}
	\norm{v}_{W^{k,p}(a,b)} \coloneqq \left(\sum\limits_{i = 0}^k \norm{v^{(i)}}_{L^p(a,b)}^p\right)^{1/p} \label{eq:sobolev1_norm}
	\end{equation}
	In the case of $p=2$ the notation $H^k(a,b) \coloneqq W^{k,2}(a,b)$ is used. This space is a Hilbert Space with the following inner product:
	\begin{equation}
	(v,z)_{H^k(a,b)} \coloneqq \sum_{i=0}^{k} (v^{(k)}, z^{(k)})_{L^2(a,b)} = \sum_{i=0}^{k} \int_a^b v^{(k)}(\tau)z^{(k)}(\tau) \dif \tau \label{eq:sobolev1_inner_product}
	\end{equation}
	These results can be found in the literature, for example in Mitrović and Žubrinić \cite[Chapter 5, Section 2, Remark 2]{mitrovic1997fundamentals}.
\end{definition}

This definition of the Sobolev Spaces, although the most simple, is not the one most commonly found in the literature. The usual way to define a weak derivative, and therefore the Sobolev Spaces is through testing with infinitely differentiable functions with compact support. This alternative definition will be useful down the line, as it is very analogous to the variational inequalities we will be considering:

\begin{lemma} Equivalent Characterization of the Sobolev Spaces \newline
	Let $C_c^\infty(a,b)$ be the space of the infinitely differentiable functions with compact support inside $(a,b)$. Note that this implies that these functions disappear on the boundary. Let $k \in \mathbb{N}_0$. Let $v$ be a locally integrable function. A locally integrable function $w$, which fulfills:
	\begin{equation} \label{eq:weak_derivative_through_testing}
	\int_a^b u \varphi^{(k)} \dif x = \int_a^b w \varphi \dif x \quad \forall \varphi \in C_c^\infty(a,b)
	\end{equation}
	or, equivalently:
	\begin{equation}
		(u,\varphi^{(k)})_{L^2(a,b)} = (w,\varphi)_{L^2(a,b)} \quad \forall \varphi \in C_c^\infty(a,b)
	\end{equation}
	is the $k$th weak derivative of $v$, i.e. $v^{(k)} = w$. With this definition one can also see through partial integration that in the case that a classical derivative exists, it is also the weak derivative.
	
	 furthermore $p\in [1,\infty]$. For $k \in \mathbb{N}_0$ and $p \in \mathbb{N}$, the Sobolev Space $W^{k,p}(a,b)$ is the space of all $L^p(a,b)$-functions whose $i$th partial derivative, defined in the above sense, has a finite $L^p$-norm, for all $i \in \{0,\dots,k\}$. 
	
	
	Again, this definition can be found in the literature, for example in Kurdilla and Zabarankin \cite[Definition 3.2.3]{kurdila2005convex}.
\end{lemma}

The Sobolev Spaces are subspaces of the $L^p$-spaces, therefore their elements are equivalence classes of Lebesgue-almost everywhere equal functions. In order to formulate the Signorini Problem weakly, we however need to enforce the boundary conditions, which are defined on the two points of $\partial \Omega$. We will show that in the case of $H^k(a,b)$, there is always a canonical representative in $C^{k-1}([a,b])$.


\begin{lemma}\thlabel{thm:sobolev_embedding_1} Sobolev Embedding for $H^1(a,b)$ \newline
	For every $u \in H^1(a,b)$ there exists a $v \in C^{0,\frac{1}{2}}([a,b])$, where $C^{0,\frac{1}{2}}$ is a Hölder-Space with Hölder-continuity constant $\frac{1}{2}$, such that $u = [v \vert_{(a,b)}]$. As an embedding, this can be written as:
	\begin{equation}
	H^1(a,b) \hookrightarrow C^{0,\frac{1}{2}}([a,b])
	\end{equation}
	\begin{proof}
	Let $u \in H^1(a,b)$, $a \leq x_1 < x_2 \leq b$, and let $u = [v]$ such that $v(x) = v_0 + \int_a^x w(\tau) \dif \tau$. We will show that $v$ is $\frac{1}{2}$-Hölder-continuous:
	\begin{align*}
	\vert v(x_1) - v(x_2) \vert &= \left\vert \int_{a}^{x_1} w(\tau) \dif \tau - \int_{a}^{x_2} w(\tau) \dif \tau \right\vert \\
	&= \left\vert \int_{x_1}^{x_2} w(\tau) \dif \tau \right\vert \overset{\textnormal{Hölder}}{\leq} \Vert w \Vert_{L^2} \left\vert \int_{x_1}^{x_2} 1 \dif \tau \right\vert ^{\frac{1}{2}} \\
	& = \Vert w \Vert_{L^2} \vert x_1 - x_2 \vert ^{\frac{1}{2}}
	\end{align*}
	\end{proof}
\end{lemma}

\begin{corollary} Sobolev Embedding for $H^k(a,b)$ \newline
		For every $u \in H^k(a,b)$ there exists a $v \in C^{k-1}([a,b])$, such that $u = [v \vert_{(a,b)}]$. As an embedding, this can be written as:
	\begin{equation}
	H^k(a,b) \hookrightarrow C^{k-1}([a,b])
	\end{equation}
	\begin{proof}
		This follows directly from \thref{thm:sobolev_embedding_1} and the Fundamental Theorem of Calculus.
	\end{proof}
\end{corollary}
	

Using these statements, we will henceforth identify elements of $H^k$ with their continuous representatives, allowing for the following statements involving pointwise conditions on elements of this space to be well-defined.

\begin{theorem} Weak Formulation of the Signorini Problem\newline
	Let $K \coloneqq \left\{ v\in H^1(-1,1) \,|\, v(-1) \geq 0, v(1) \geq 0 \right\}$. Then a solution $u \in K$ of the following variational inequality is a weak solution of \eqref{eq:signorini}:
	\begin{equation}
	\int_{-1}^1 u'(v'-u') + u(v-u) - f(v-u) \dif x \geq 0 \quad \forall v \in K \label{eq:variational_inequality}
	\end{equation}
	Taking the definition of the inner product in the space $H^1$ into account \eqref{eq:sobolev1_inner_product}, this can be equivalently written as:
	\begin{equation}
	(u,v-u)_{H^1(\Omega)} \geq (f,v-u)_{L^2(\Omega)} \quad \forall v \in K
	\end{equation}
	\begin{proof}
	Consider the differential form of the Signorini Problem \eqref{eq:signorini} \eqref{eq:boundary_signorini} and suppose $u \in C([-1,1])$ solves it classically. Furthermore let $v \in K$. Multiplying \eqref{eq:signorini} by the test function $v-u$ and integrating over the domain $\Omega$ yields:
	\begin{align*}
	&\int_{-1}^1 {(-u'' + u - f)(v - u)\dif x} = 0  \\
	\Longleftrightarrow &\int_{-1}^1 -u''(v-u) \dif x + \int_{-1}^1 u(v-u) - f (v-u) \dif x =0
	\end{align*}
	The right term is already in the required form. We now integrate the left term by parts to remove the second derivative:
	\begin{align}
	\Longleftrightarrow \underbrace{\left. -u'(v-u) \right|_{-1}^{1}}_{\textnormal{(}\mytag{*}{eq:boundary_int_term}\textnormal{)}} + \int_{-1}^1{u'(v'-u') + u(v-u) - f(v-u) \dif x} = 0 \label{eq:variational_equality}
	\end{align}
	We resolve the boundary term from the integration by parts with the boundary conditions on the Signorini Problem \eqref{eq:boundary_signorini}:
	\begin{align*}
	\eqref{eq:boundary_int_term} &= -u'(1)v(1) + \underbrace{u'(1)u(1)}_{\overset{\eqref{eq:boundary_signorini}}{=} 0} + u'(-1)v(-1) - \underbrace{u'(-1)u(1)}_{\overset{\eqref{eq:boundary_signorini}}{=} 0} \\
	&= -u'(1)v(1) + u'(-1)v(-1) \overset{\eqref{eq:boundary_signorini}}{\leq} 0
	\end{align*}
	Enforcing this inequality on \eqref{eq:variational_equality} results in the desired variational inequality \eqref{eq:variational_inequality}.
	
	We now observe that the derived formulation of the problem does not need the assumption of $u \in C([-1,1])$ and can be more generally defined for $u \in K$. Hence this is the weak formulation of the problem.
	
	\end{proof}
\end{theorem}

We will now formulate this problem as a convex minimization problem in the Sobolev Banach Space. This will be useful when considering the existence and uniqueness of solutions, as it will allow for the use of standard techniques of convex optimization.

\begin{lemma} The functional to minimize is convex \newline
	Define the functional $F$ as follows:
	\begin{equation}
			F(v) \coloneqq \int_{-1}^1{\frac{1}{2} (v')^2 + \frac{1}{2} v^2 - fv \dif x}
	\end{equation}
	This functional is uniformly (therefore strictly) convex.
	\begin{proof}
		Firstly, we note that $F(v) = \frac{1}{2} \norm{v}_{H^1}^2 - (f,v)_{L^2} $. It therefore suffices to show that $v \mapsto \norm{v}_{H^1}^2$ is uniformly convex, as affine transformations do not alter it. We will show that this is true for every scalar-product induced norm in a space $(X, (\cdot,\cdot))$. To that avail, let $v,w \in X$, $v \neq w$ and $\lambda \in (0,1)$:
		\begin{align*}
		(1-\lambda)\norm{v}^2 &+ \lambda\norm{w}^2 - \norm{(1-\lambda)v - \lambda w}^2 \\
		& = (1-\lambda) (v,v) + \lambda (w,w) - (((1-\lambda)v,(1-\lambda)v) + 2((1-\lambda)v, \lambda w)+ (\lambda w, \lambda w)) \\
		&= (1-\lambda) (v,v) - (1-\lambda)^2 (v,v) - 2 (1-\lambda)\lambda(v,w) + \lambda (w,w) - \lambda^2 (w,w) \\
		&= (1-\lambda)(1-(1-\lambda))(v,v)- 2 (1-\lambda)\lambda(v,w) + \lambda (1-\lambda)(w,w) \\
		&= \lambda(1- \lambda)((v,v) - 2(v,w)+(w,w)) = \lambda(1-\lambda)(v-w,v-w) \\
		&= \lambda(1-\lambda)\norm{v-w}^2
		\end{align*}
		This is exactly the definition with modulus of convexity $\phi(\alpha) = \alpha ^2$.
		
	\end{proof}
\end{lemma}

\begin{theorem} Equivalent formulation as a convex minimization problem\newline
	A solution $u \in H^1(-1,1)$ of the following minimization problem is a weak solution of \eqref{eq:signorini}:
	\begin{equation} \label{eq:minimization_problem}
	\min_{\substack{v \in H^1(-1,1) \\ v(-1) \geq 0,\, v(1) \geq 0}} F(v)
	\end{equation}
	The functional $F$ can be understood as the energy functional. This is a convex problem.
	\begin{proof}
	We start with the variational inequality \eqref{eq:variational_inequality}:
	\begin{align*}
		&\int_{-1}^1 u'(v'-u') + u(v-u) - f(v-u) \dif x \geq 0 \quad \forall v \in K \\
		\Longleftrightarrow & \int_{-1}^1 u'v' - (u')^2 + uv - u^2 - fv - fu \dif x \geq 0 \quad \forall v \in K \\
	\end{align*}
	We will now use the Young inequality to bound the integral from above:
	\begin{equation}
	ab \leq \frac{1}{2} a^2 + \frac{1}{2} b^2 \label{eq:binomial_inequality}
	\end{equation}
	This follows directly from the binomial formula. Using it on the variational inequality yields the following:
	\begin{align*}
	&\int_{-1}^1 \frac{1}{2} (u')^2 + \frac{1}{2} (v')^2 - (u')^2 + \frac{1}{2} u^2 + \frac{1}{2} v^2 - u^2 - fv - fu \dif x \geq 0 \quad \forall v \in K \\
	\Longleftrightarrow & \int_{-1}^1 \frac{1}{2} (v')^2 + \frac{1}{2} v^2 - fv \dif x \geq \int_{-1}^1 \frac{1}{2} (u')^2 + \frac{1}{2} u^2 - fu \dif x \quad \forall v \in K
	\end{align*}
	Or, equivalently:
	\begin{equation*}
	F(v) \geq F(u) \quad \forall v \in K 
	\quad \Longleftrightarrow \quad u = \argmin_{\substack{v \in H^1(-1,1) \\ v(-1) \geq 0,\, v(1) \geq 0}} F(v)
	\end{equation*}
	
	For the other direction of the proof let $u$ be a solution of \eqref{eq:minimization_problem}. As $u$ is a minimizer, it follows for every $t \geq 0$ and every $v \in K$:
	\begin{equation*}
	\frac{F(u+t(v-u)) - F(u)}{t} \geq 0
	\end{equation*}
	Starting from there, we will derive the variational inequality by taking the limit $t\rightarrow 0$:
	\begingroup
	\allowdisplaybreaks
	\begin{align*}
	&\lim\limits_{t \rightarrow 0} \frac{1}{t} \Bigg( 
	\int_{-1}^{1} \frac{1}{2} (u'+t(v'-u'))^2 + \frac{1}{2} (u+t(v-u))^2 - f(u+t(v-u))\\
	 &\phantom{\lim\limits_{t \rightarrow 0} \frac{1}{t} \Bigg( 
		\int_{-1}^{1}} -\frac{1}{2}(u')^2  - \frac{1}{2}u^2 + fu \dif x \Bigg) \\
	= &\lim\limits_{t \rightarrow 0} \int_{-1}^1 \frac{1}{2} t(v'-u')^2 + u'(v'-u') + \frac{1}{2}t(v-u)^2 + u(v-u) -f(v-u) \dif x \\
	= & 	\int_{-1}^1 u'(v'-u') + u(v-u) - f(v-u) \dif x \geq 0 \quad \forall v \in K
	\end{align*}
	\endgroup
	This is exactly the required form, and the equivalence of the two formulations follows.
	\end{proof}
\end{theorem}

\begin{theorem}\thlabel{thm:unique_solution_signorini} Existence of a unique solution to the Signorini Problem \newline
	The weak formulation of the Signorini Problem \eqref{eq:minimization_problem} has a unique solution $u \in H^1(-1,1)$
	\begin{proof}
		We will prove this using the Direct Method in the Calculus of Variations. In a first step, we will show that the functional $F$ is bounded from below. To this avail define $\{v_n\}_{n\in \mathbb{N}} \subset H^1(\Omega)$ such that: 
		\begin{equation*}
		v_n(\pm 1) \geq 0\textnormal{,} \quad \lim\limits_{n \rightarrow \infty} F(v_n) = \inf_{\substack{v \in H^1(-1,1) \\ v(-1) \geq 0,\, v(1) \geq 0}} F(v)
		\end{equation*} 
		Then there exists a constant $C$ such that:
		\begin{align*}
		C \geq F(v_n) \overset{\eqref{eq:minimization_problem}}&{=} \frac{1}{2} \norm{v_n}_{H^1}^2 - (f,v)_{L^2} = \frac{1}{2} \norm{v_n}_{H^1}^2 - \left(4f,\frac{1}{4}v\right)_{L^2} \\
		&\geq \frac{1}{2}\norm{v_n}_{H^1}^2 - \frac{1}{4} \norm{v_n}_{L^2}^2 - 4 \norm{f}_{L^2}^2 \\
		&\geq \frac{1}{4} \norm{v_n}_{H^1}^2 - 4 \norm{f}_{L^2}^2
		\end{align*}
		\begin{equation} \label{eq:H1_bound_by_f}
		\Longrightarrow \norm{v_n}_{H^1}^2 \leq 4\left(C + 4 \norm{f}_{L^2}^2\right) \eqqcolon \tilde{C}
		\end{equation}
		The infimal sequence is bounded and therefore also the functional. Now we will use the Functional Analysis result, that a bounded sequence in a Hilbert Space has a weakly convergent subsequence. This can be found for example in Royden and Fitzpatrick \cite[Chapter 16, Theorem 6]{royden2010real}. On top of that, we use that a convex, continuous functional is weakly lower semi-continuous, as can be found in Kurdila and Zabarankin \cite[Theorem 7.2.5]{kurdila2005convex}. 
		
		Let $\{\tilde{v}_n\}_{n\in \mathbb{N}} $ be the weakly convergent subsequence such that $\tilde{v}_n \rightharpoonup u$. Then the claim follows, by definition of weak convergence and lower semi-continuity:
		\begin{equation*}
		\inf_{\substack{v \in H^1(-1,1) \\ v(-1) \geq 0,\, v(1) \geq 0}} F(v) =  \liminf\limits_{n\rightarrow\infty} F(\tilde{v}_n) \geq F(u)
		\end{equation*}
		
		The uniqueness of the solution follows from the strict convexity of the functional \cite{aubin79}.
	\end{proof}
\end{theorem}

Now we will see that without any further assumptions, the weak solution to the Signorini Problem is actually in $H^2$ and its norm is bound by the right hand side $f$.

\begin{theorem} $H^2$-regularity of the solution to the Signorini Problem \newline
	Let $u \in H^1(\Omega)$ be the solution to \eqref{eq:minimization_problem}. Then the following holds:
	\begin{align}
		u &\in H^2(\Omega) \label{eq:H2_regularity} \\
		\norm{u}_{H^2(\Omega)} &\leq C\norm{f}_{L^2(\Omega)} \label{eq:H2_norm_bound_by_f} 
	\end{align}
	for $C \in \mathbb{R}_{>0}$.
	\begin{proof}
		$u \in H^1(\Omega)$ be the solution to \eqref{eq:minimization_problem}. Then it is also the solution of \eqref{eq:variational_inequality} and therefore fulfills the following:
		\begin{equation*}
		(u,v-u)_{H^1(\Omega)} \geq (f,v-u)_{L^2(\Omega)} \quad \forall v \in K
		\end{equation*}
		Let $\varphi \in C_c^\infty(\Omega)$, and let $v_1 = u + \varphi$ and $v_2 = u - \varphi$. Since $u$ is a solution and therefore fulfills the boundary conditions and $\varphi$ disappears on the boundary, $v_1,v_2 \in K$. Testing with both in the variational inequality, as $\varphi$ was arbitrary, yields the following:
		\begin{equation*}
			(u, \varphi)_{H^1(\Omega)} = (f, \varphi)_{L^2(\Omega)} \quad \forall \varphi \in C_c^\infty(\Omega)
		\end{equation*}
		We will transform this to get the characterization of $H^2$-regularity \eqref{eq:weak_derivative_through_testing}:
		\begin{align*}
		&(u,\varphi)_{H^1(\Omega)} \overset{\eqref{eq:sobolev1_inner_product}}{=} (u,\varphi)_{L^2(\Omega)} + (u',\varphi')_{L^2(\Omega)} = (f, \varphi)_{L^2(\Omega)} \quad \forall \varphi \in C_c^\infty(\Omega) \\
		\Longleftrightarrow \quad &(u',\varphi')_{L^2(\Omega)} = (f-u,\varphi)_{L^2(\Omega)} \quad \forall \varphi \in C_c^\infty(\Omega)
		\end{align*}
		Which means that $u' \in H^1(\Omega)$, with $(u')' = f-z$, implying $u \in H^2(\Omega)$.
		
		Using $u'' = f-z$, the characterizing equation of the weak derivative and \eqref{eq:H1_bound_by_f} we obtain:
		\begin{align*}
			\norm{u}_{H^2(\Omega)}\overset{\eqref{eq:sobolev1_norm}}&{=} \norm{u}_{H^1(\Omega)} + \norm{u''}_{L^2(\Omega)} \\ 
			\overset{\substack{\text{Cauchy} \\ \text{Schwarz}}}&{\leq} 2\norm{u}_{H^1(\Omega)} + \norm{f}_{L^2(\Omega)} \\
			&\overset{\eqref{eq:H1_bound_by_f}}{\leq} C \norm{f}_{L^2(\Omega)}
		\end{align*}
	\end{proof}
\end{theorem}


\chapter{Discretization}
We will now consider a discretization of the Signorini Problem. To that end, we will restrict the solution space to $V_h \coloneqq \{z \in C([-1,1]) \, | \, z \,\, \text{piecewise affine subordinate to a partition} \\ \mathmbox{-1}=x_0<x_1<\dots<x_N = 1 \}$. Furthermore, we will assume a uniform partition with $h = x_{i+1} - x_i$.

\begin{definition} Finite Element Space
We define a basis for the finite element space $V_h$, which are the piecewise linear functions with a bump in the nodes:
\begin{align} \label{eq:basis_finite_element_space}
\begin{split}
V_h = \mathrm{span}\,\Phi_h &= \mathrm{span}\{\varphi_i^h \,|\, i \in \{0,\dots,N\}\} \\
\varphi_i^h (x) &= \begin{cases}
\frac{x-x_{i-1}}{h} & \text{if} \quad x\in [x_{i-1},x_i] \\
\frac{x_{i+1}-x}{h} & \text{if} \quad  x\in [x_i,x_{i+1}] \\
0 & \text{if} \quad  \abs{x-x_i} > h
\end{cases}
\end{split}
\end{align}
\end{definition}

\begin{definition} The Discrete Signorini Problem \newline
	Using this discretization, \eqref{eq:minimization_problem} turns into:
	\begin{equation} \label{eq:discrete_minimization}
	\min_{\substack{v_h \in V_h \\ v_h(-1) \geq 0,\, v_h(1) \geq 0}} F(v_h) = \min_{\substack{v_h \in V_h \\ v_h(-1) \geq 0,\, v_h(1) \geq 0}} \int_{-1}^1{\frac{1}{2} (v_h')^2 + \frac{1}{2} v_h^2 - fv_h \dif x} 
	\end{equation}
	Or, equivalently, as a variational inequality:
	\begin{equation}\label{eq:discrete_variational_inequality}
	u_h \in V_h,\quad (u_h, v_h-u_h)_{H^1(\Omega)} \geq (f, v_h - u_h)_{L^2(\Omega)} \quad \forall v_h \in V_h \cap K
	\end{equation}
	
\end{definition}

We could use the same techniques from the previous section to show that this problem always admits a unique solution. However, it will be much more obvious if we rewrite it in the usual finite dimensional form. This will also be the form used to implement a solver.

\begin{lemma} Matrix Formulation of the Discrete Problem \newline
	Using the basis \eqref{eq:basis_finite_element_space}, we represent the solution function $v_h$ as a linear combination of the basis functions and define $\mathbf{v}$ as the corresponding coefficient vector:
	\begin{equation} \label{eq:basis_representation_v_h}
	v_h(x) = \sum_{i=1}^{N} \mathbf{v}_i \varphi_i^h(x)
	\end{equation}
		The above problem \eqref{eq:discrete_minimization} is then equivalent to:
		\begin{equation}
		\min_{\substack{\mathbf{v} \in \mathbb{R}^{(N+1)\times(N+1)}\\ \mathbf{v}_{0} \geq 0,\, \mathbf{v}_N \geq 0}} \frac{1}{2} \mathbf{v}^T (B_1 + B_2) \mathbf{v} - \mathbf{f}^T  \mathbf{v}
		\end{equation}
		We call $B_1$ the \emph{mass matrix} and $B_2$ the \emph{stiffness matrix}. $B_1$, $B_2$ and $\mathbf{f} \in \mathbb{R}^{(N+1)\times(N+1)}$ are defined as follows:
		\begin{align*}
		B_1 &\coloneqq 			h	\begin{pmatrix}
		1/3& 1/6 & & & & & \\
		1/6& 2/3& 1/6 & & & & \\
		& 1/6& 2/3& 1/6 & & & \\
		& & & \ddots & & & \\
		& & & 1/6& 2/3& 1/6 & \\
		& & & & 1/6& 2/3& 1/6 \\
		& & & & & 1/6& 1/3
		\end{pmatrix}, \\
		 B_2 &\coloneqq \frac{1}{h} 			\begin{pmatrix}
		1& -1 & & & & & \\
		-1& 2& -1 & & & & \\
		& -1& 2& -1 & & & \\
		& & & \ddots & & & \\
		& & & -1& 2& -1 & \\
		& & & & -1& 2& -1 \\
		& & & & & -1& 1
		\end{pmatrix} \\
		\\
		\mathbf{f}_i &\coloneqq \phantom{h} \left(\int_{-1}^1 f\varphi_i^h\dif x\right)
		\end{align*}
		Note that only $\mathbf{f}$ changes with the choice of $f$, the stiffness and mass matrices are therefore structural to the problem. In practice, as portrayed in the implementation section, the vector $\mathbf{f}$ can only be approximated numerically with a quadrature of high enough order.
		\begin{proof}
			We will analyze \eqref{eq:discrete_minimization} in three parts. For the mass and stiffness matrices we will decompose the integration interval into the intervals $[t_{i-1},t_i]$, so that we can consider only the two basis functions corresponding to the left and right node of the interval:
			\begin{align*}
				\int_{-1}^1 v_h^2 \dif x &= \sum_{i=1}^{N} \int_{t_{i-1}}^{t_i} (\mathbf{v}_i \varphi_i + \mathbf{v}_{i-1} \varphi_{i-1})^2 \dif x\\
				&= \sum_{i=1}^{N} \int_{t_{i-1}}^{t_i} \left(\frac{x-x_{i-1}}{h}\mathbf{v}_i + \frac{x_i-x}{h}\mathbf{v}_{i-1}\right)^2 \dif x \\
				&= \sum_{i=1}^{N} \int_{t_{i-1}}^{t_i} \left(\frac{x-x_{i-1}}{h}\right)^2 \mathbf{v}_i^2 + 2\left(\frac{x-x_{i-1}}{h}\right) \mathbf{v}_i \left(\frac{x_i-x}{h}\right) \mathbf{v}_{i-1} + \left(\frac{x_i-x}{h}\right)^2 \mathbf{v}_{i-1}^2 \dif x \\
				&= \sum_{i=1}^{N} \frac{1}{3}h \mathbf{v}_i^2 + 2\frac{1}{6}h\mathbf{v}_i \mathbf{v}_{i-1} + \frac{1}{3} h \mathbf{v}_{i-1}^2 \\
				&= h \begin{pmatrix}
					\mathbf{v}_0 \\					
					\mathbf{v}_1 \\
					\vdots \\
					\mathbf{v}_N 
				\end{pmatrix}^T
				\begin{pmatrix}
				1/3& 1/6 & & & & & \\
				1/6& 2/3& 1/6 & & & & \\
				& 1/6& 2/3& 1/6 & & & \\
				& & & \ddots & & & \\
				& & & 1/6& 2/3& 1/6 & \\
				& & & & 1/6& 2/3& 1/6 \\
				& & & & & 1/6& 1/3
				\end{pmatrix}
				\begin{pmatrix}
				\mathbf{v}_0 \\					
				\mathbf{v}_1 \\
				\vdots \\
				\mathbf{v}_N 
				\end{pmatrix} = \mathbf{v}^T B_1 \mathbf{v}
			\end{align*}
			We proceed similarly for the stiffness matrix, here differentiating the two basis functions before integrating $v_h$:
			\begin{align*}
			\int_{-1}^1 (v_h')^2 \dif x &= \sum_{i=1}^{N} \int_{t_{i-1}}^{t_i} \left(\frac{x_i-x_{i-1}}{h}\right)^2 \dif x \\
			&= \sum_{i=1}^{N} \frac{1}{h} \mathbf{v}_i^2 - 2\frac{1}{h} \mathbf{v}_i\mathbf{v}_{i-1} + \frac{1}{h}\mathbf{v}_{i-1}^2 \\
			&= \frac{1}{h} 
			\begin{pmatrix}
			\mathbf{v}_0 \\					
			\mathbf{v}_1 \\
			\vdots \\
			\mathbf{v}_N 
			\end{pmatrix}^T
			\begin{pmatrix}
			1& -1 & & & & & \\
			-1& 2& -1 & & & & \\
			& -1& 2& -1 & & & \\
			& & & \ddots & & & \\
			& & & -1& 2& -1 & \\
			& & & & -1& 2& -1 \\
			& & & & & -1& 1
			\end{pmatrix}
			\begin{pmatrix}
			\mathbf{v}_0 \\					
			\mathbf{v}_1 \\
			\vdots \\
			\mathbf{v}_N 
			\end{pmatrix} = \mathbf{v}^T B_2 \mathbf{v}
			\end{align*}
			Finally, we derive the vector $\mathbf{f}$, for which we will use the linearity of $v_h$ in the third part of \eqref{eq:discrete_minimization}:
			\begin{align*}
				\int_{-1}^1 fv_h \dif x &= \int_{-1}^1 f \sum_{i=0}^{N} \mathbf{v}_i \varphi_i^h \dif x \\
				&= \sum_{i=0}^{N} \mathbf{v}_i \left(\int_{-1}^1 f\varphi_i^h\dif x\right) = \mathbf{f}^T \mathbf{v}
			\end{align*}
		\end{proof}
\end{lemma}

\begin{corollary}\thlabel{thm:ex_uniq_discrete_sol} Existence and Uniqueness of the Discrete Solution \newline
	The discrete probelm \eqref{eq:discrete_minimization} always has a unique solution.
	\begin{proof}
		This follows directly from the previous lemma, as $B_1 + B_2$ is a positive definite matrix. The proof of this result can be found in standard literature, such as \cite{ulbrich2012nichtlineare}.
	\end{proof}
\end{corollary}

\chapter{Error Estimates}
The goal of this chapter is to analyze how good the solution to the discrete problem \eqref{eq:discrete_minimization} approximates the solution of the continuous one \eqref{eq:minimization_problem}. We will analyze the convergence order as a function of the mesh width $h$ with respect to several norms, and show that these convergence orders are optimal, as they are of the same order as the interpolation error for the same equidistant nodes, c.f. \cite{brenner2002mathematical}. 

For the entirety of this chapter, let $u \in H^2(\Omega)$ be the unique solution of \eqref{eq:variational_inequality} for a fixed $f \in L^2(\Omega)$. Furthermore, let $u_h \in V_h$ be the solution of \eqref{eq:discrete_variational_inequality} for the same $f$. Note that this implies that all constants mentioned in the chapter depend on the choice of $f$. Specifically, they all depend on the $L^2$-norm of $f$.

\section{The Ritz Projection}
In order to analyze the discretization error, it comes naturally to mind to estimate it by the least possible distance to a finite element function, by means of the Ritz Projection, and using the triangle inequality, subsequently to the solution of the discrete problem.

\subsection{Definition}

\begin{definition} The Ritz Projection \newline
	The Ritz Projection $R_h : H^1(\Omega) \rightarrow V_h$ is the orthogonal projection from $H^1(\Omega)$ to the subspace $V_h$, as uniquely defined by:
	\begin{equation}\label{eq:Ritz_Projection}
	(u-R_h(u), v_h)_{H_1} = 0 \quad \forall v_h \in V_h
	\end{equation}
	Because of the orthogonality, $R_h(u)$ is the solution to the following minimization problem:
	\begin{equation}\label{eq:ritz_minimization}
	R_h(u) = \argmin_{\substack{v_h \in V_h}} \norm{v_h - u}_{H_1}
	\end{equation}
\end{definition}
We will therefore derive an upper bound for the error between the Ritz Projection of the solution $u$ and the discrete solution $v_h$. To that avail, following the lines of \cite{2019christof}, we will firstly formulate the Ritz Projection in a way that links it to the solution of the Signorini Problem:

\begin{lemma} Ritz Projection of a Signorini Solution \newline
	We define $R_h(u)$ to be the unique solution $\tilde{u}_h$ of the following variational inequality:
	\begin{equation}\label{eq:ritz_variational}
	(\tilde{u}_h, v_h-\tilde{u}_h)_{H^1(\Omega)} \geq (f, v_h-\tilde{u}_h)_{L^2(\Omega)} \quad \forall v_h \in \tilde{K}_h
	\end{equation}
	where $\tilde{K}_h$ is defined as follows:
	\begin{align*}
	\tilde{K}_h \coloneqq \{v_h \in V_h \,|\,  v_h(1) &\geq (R_h(u) - u)(1) \quad \text{and} \\
	 v_h(-1) &\geq (R_h(u) - u)(-1)\}
	\end{align*}
	\begin{proof}
		The variatonal inequality \eqref{eq:ritz_variational} admits a unique solution. This follows in exactly the same way as \thref{thm:ex_uniq_discrete_sol}, as this problem has the same matrix representation, only with different boundary conditions. This variational inequality is equivalent to the same. To show that this is indeed $R_h(u)$, note that $R_h(u) \in \tilde{K}_h$, as $u$ is nonnegative in $\partial\Omega$. It therefore suffices to show that $R_h(u)$ fulfills the inequality. Let $v_h \in \tilde{K}_h$, then:
		\begin{align*}
		(R_h(u), v_h - R_h(u))_{H^1(\Omega)} \overset{\eqref{eq:Ritz_Projection}}&{=} (u, v_h - R_h(u))_{H^1(\Omega)} \\
		&= (u, v_h - R_h(u) + u - u)_{H^1(\Omega)} \\
		\overset{\eqref{eq:variational_inequality}}&{\geq} (f, v_h - R_h(u) + u - u)_{L^2(\Omega)} \\
		&= (f, v_h - R_h(u))_{L^2(\Omega)}
		\end{align*}
	\end{proof}
\end{lemma}


\subsection{Supercloseness}

With this representation we can conclude the following supercloseness result: \textcolor{red}{Insert here some text about supercloseness}

\begin{lemma} Supercloseness Lemma \newline
	The $H^1$-error between $u_h$ and the Ritz Projection of $u$ is bounded as follows:
	\begin{align} \label{eq:supercloseness_lemma}
	\begin{split}
	&\norm{u_h - R_h(u)}_{H^1(\Omega)} \\ &\leq \inf \left\{
	\norm{w_h}_{H^1(\Omega)} \,\big|\, w_h \in V_h, \quad (R_h(u) - u)(\pm 1) \leq w_h(\pm 1) \leq R_h(u)(\pm 1)
	\right\}
	\end{split}
	\end{align}
	\begin{proof}
		Let $w_h$ be in the feasible set of the right hand side (which is necessarily non-empty, since it contains $R_h(u)$). As $R_h(u)$ is the solution of \eqref{eq:ritz_variational}, we have:
		\begin{equation*}
		(R_h(u), v_h - R_h(u))_{H^1(\Omega)} \geq (f, v_h - R_h(u))_{L^2(\Omega)}
		\end{equation*}
		for all $v_h \in \tilde{K}_h$, in particular for all $v_h \in \tilde{K}_h$ with $v_h(\pm 1) \geq w_h(\pm 1)$. In particular, we may choose $v_h \coloneqq u_h + w_h$, and obtain:
		\begin{equation*}
		(R_h(u), R_h(u)-u_h)_{H^1(\Omega)} \leq (R_h(u),w_h)_{H^1(\Omega)} + (f,R_h(u)-u_h-w_h)_{L^2(\Omega)}
		\end{equation*}
		On the other hand, we also know that $R_h(u)-w_h \geq R_h(u) - R_h(u) = 0$ on $\partial\Omega = \{\pm 1\}$. Thus, we can also choose $v_h \coloneqq R_h(u) - w_h$ as a test function and obtain:
		\begin{equation*}
		(u_h, u_h-R_h(u))_{H^1(\Omega)} \leq (u_h,-w_h)_{H^1(\Omega)} + (f,u_h + w_h - R_h(u))_{L^2(\Omega)}
		\end{equation*}		
		We may now add both inequalities to obtain:
		\begin{equation*}
		\norm{R_h(u)-u_h}^2_{H^1(\Omega)} \leq (R_h(u)-u_h, w_h)_{H^1(\Omega)}
		\end{equation*}
		As $w_h$ was an arbitrary function from the feasible set of which the infimum is taken, the claim follows.
	\end{proof}
\end{lemma}

\begin{corollary} Supercloseness \newline
	Under the same conditions of the previous lemma, the following bound also holds:
	\begin{equation}\label{eq:supercloseness}
	\norm{u_h - R_h(u)}_{H^1(\Omega)} \leq \frac{7}{3} \max \left(\abs{(u-R_h(u))(-1)}, \abs{(u-R_h(u))(1)}\right)
	\end{equation}
	\begin{proof}
		Recall the bound from \eqref{eq:supercloseness_lemma}. Let $w$ be defined as the linear interpolate of $y_0 \coloneqq (R_h(u) - u)(-1)$ and $y_1 \coloneqq (R_h(u) - u)(1)$, which reads:
		\begin{equation}
		w(x) = \frac{y_0(1-x)+y_1(1+x)}{2}
		\end{equation}
		This function trivially fulfills the boundary conditions from \eqref{eq:supercloseness_lemma}, and therefore its norm bounds the infimum from above.
		Let us therefore compute the norm explicitly:
		\begin{align*}
		\norm{w}_{H^1(\Omega)}^2 &= \int_{-1}^1{\abs{w}^2 + \abs{w'}^2} \dif x\\
		&= \frac{1}{6}(7y_0^2 + 7 y_1^2 - 2y_0y_1)  \leq \frac{7}{6} (y_0^2 + y_1^2)
		\end{align*}
		With this we follow the claim:
		\begin{align*}
		\norm{w}_{H^1(\Omega)} \leq \frac{7}{6} (\abs{y_0} + \abs{y_1}) \leq \frac{7}{3} \max(\abs{y_0}, \abs{y_1})
		\end{align*}
	\end{proof}
\end{corollary}

\subsection{$L^{\infty}$ and $H^1$ Error Estimates}

Firstly we will analyze the $L^{\infty}$ error, which we will subsequently also use to estimate the $H^1$ one.

\begin{definition} Lagrange-Interpolation \newline
	For a mesh width $h$ and corresponding nodes $-1=x_0<x_1<\dots<x_N = 1$, where $h = x_{i+1} - x_i$, the Lagrange-Interpolation Operator maps a continuous function to the linear spline interpolating at these nodes. It is uniquely characterized by the value at the nodes $x_i$:
	\begin{align}
	\begin{split}
	I_h : C(\bar{\Omega}) &\rightarrow V_h \\
	I_h(u)(x_i) &= u(x_i)
	\end{split}
	\end{align}
\end{definition}

\begin{lemma} Bound for the $L^\infty$-error of the Ritz Projection \newline
	There exists a $C \in \mathbb{R}_{>0}$ such that the following holds:
	\begin{equation} \label{eq:linfty_error_lemma}
		\norm{u -R_h(u)}_{L^\infty} \leq Ch^{3/2}\norm{u}_{H^2}
	\end{equation}
	\begin{proof}
		The main idea is to introduce the Lagrange Interpolate through Minkowski and then to use the Inverse Estimate for Finite Elements, as can be found in a very general setting in \cite{brenner2002mathematical}:
		\begin{align} \label{eq:lemma6part1}
		\norm{u -R_h(u)}_{L^\infty} \overset{\text{Minkowski}}&{\leq} \hspace{0.8em} \norm{u -I_h(u)}_{L^\infty} +\norm{I_h(u) -R_h(u)}_{L^\infty} \nonumber \\
		\overset{\text{\cite[Thm 4.5.11]{brenner2002mathematical}}}&{\leq}
		\norm{u -I_h(u)}_{L^\infty} + C_1 h^{-1/2}\norm{I_h(u) -R_h(u)}_{L^2} \\
		\overset{\text{Minkowski}}&{\leq} \hspace{0.8em} \norm{u -I_h(u)}_{L^\infty} + C_1 h^{-1/2}\norm{I_h(u) -u}_{L^2} + C_1 h^{-1/2}\norm{u -R_h(u)}_{L^2} \nonumber
		\end{align}
		We now need a suitable estimate for these three parts:
		\begin{align}\label{eq:lemma6part2}
		\begin{split}
			\norm{u -I_h(u)}_{L^\infty} \overset{\text{\cite[Thm 4.4.20]{brenner2002mathematical}}}&{\leq} C_2 h^{3/2}\norm{u}_{H^2} \\
			\norm{u -I_h(u)}_{L^2 \phantom{2}} \overset{\text{\cite[Thm 4.4.20]{brenner2002mathematical}}}&{\leq} C_3 h^2\norm{u}_{H^2} \\
			\norm{u -R_h(u)}_{L^2 \phantom{2}} \overset{\text{\cite[Lemma 3.2]{2019christof}}}&{\leq} C_4 h^2\norm{u}_{H^2} 
		\end{split}
		\end{align}
		Combining \eqref{eq:lemma6part1} and \eqref{eq:lemma6part2} we obtain the desired result:
		\begin{align*}
		\norm{u -R_h(u)}_{L^\infty} &\leq 	C_2 h^{3/2}\norm{u}_{H^2} + C_1 C_3 h^{-1/2}h^2\norm{u}_{H^2} + C_1 C_4 h^{-1/2}h^2\norm{u}_{H^2} \\
		&\leq Ch^{3/2}\norm{u}_{H^2}
		\end{align*}
	\end{proof}
\end{lemma}

\begin{theorem} $L^\infty$ Error Estimate of the Discrete Solution \newline
	There exists a $C \in \mathbb{R}_{>0}$ such that the following holds:
	\begin{equation}
		\norm{u-u_h}_{L^\infty(\Omega)} \leq C h^{3/2}
	\end{equation}
\begin{proof}
	As one might imagine from the previous results, we will introduce the Ritz projection through the triangle inequality, and then estimate the error from it to both the continuous and the discrete result:
	\begin{align*}
	\norm{u-u_h}_{L^\infty(\Omega)} &\leq \norm{u-R_h(u)}_{L^\infty(\Omega)} + \norm{R_h(u) - u_h}_{L^\infty(\Omega)} \\
	\overset{\eqref{eq:supercloseness} \eqref{eq:linfty_error_lemma}}&{\leq} 
	C_1 h^{3/2}\norm{u}_{H^2(\Omega)} + C_2 \max \left(\abs{(u-R_h(u))(-1)}, \abs{(u-R_h(u))(1)}\right) \\
	&\leq C_1 h^{3/2}\norm{u}_{H^2(\Omega)} + C_2 \norm{u-R_h(u)}_{L^\infty(\Omega)} \\
	\overset{\eqref{eq:linfty_error_lemma}}&{\leq} C_1 h^{3/2}\norm{u}_{H^2(\Omega)}  + C_3 h^{3/2} \norm{u}_{H^2(\Omega)}
	\\
	\overset{\eqref{eq:H2_norm_bound_by_f}}&{\leq} C h^{3/2} \norm{f}_{L^2(\Omega)}
	\end{align*}
	Since $f$ was fixed, the result follows.
\end{proof}
\end{theorem}

The $H^1$ error follows directly from the above bound and the interpolation error:

\begin{theorem}$H^1$ Error Estimate of the Discrete Solution \newline
	There exists a $C \in \mathbb{R}_{>0}$ such that the following holds:
	\begin{equation} \label{eq:H1_error}
		\norm{u-u_h}_{H^1(\Omega)} \leq C h
	\end{equation}
	\begin{proof}
		\begin{align*}
		\norm{u-u_h}_{H^1(\Omega)} &\leq \norm{u-R_h(u)}_{H^1(\Omega)} + \norm{R_h(u) - u_h}_{H^1(\Omega)} \\
		\overset{\eqref{eq:supercloseness}}&{\leq} \norm{u-R_h(u)}_{H^1(\Omega)} + C_1 \max \left(\abs{(u-R_h(u))(-1)}, \abs{(u-R_h(u))(1)}\right) \\
		&\leq \norm{u-R_h(u)}_{H^1(\Omega)} + C_1 \norm{u-R_h(u)}_{L^\infty(\Omega)} \\
		\overset{\eqref{eq:ritz_minimization} \eqref{eq:linfty_error_lemma}}&{\leq} \norm{I_h(u) - R_h(u)}_{H^1(\Omega)} + C_1 h^{3/2} \norm{u}_{H^2(\Omega)} \\
		\overset{\text{\cite[Thm 4.4.20]{brenner2002mathematical}}}&{\leq} C_2 h + C_1 h^{3/2}\norm{u}_{H^2(\Omega)} \\
		\overset{\eqref{eq:H2_norm_bound_by_f}}&{\leq} C h \norm{f}_{L^2(\Omega)}
		\end{align*}
		Since $f$ was fixed, the result follows.
	\end{proof}
\end{theorem}

\section{$L^2$ Error Through Duality}

The estimation of the $L^2$ error is more involved. We will follow the lines of \cite{2019christof}, which in turn follows \cite[Section 7]{mosco2006}, who proposed this approach for a similar obstacle problem. The key idea is to formulate two so-called dual variational inequalities, one for the positive component $\max(0, u-u_h)$ and one for the negative component $\min(0, u-u_h)$ of the error. From these we will derive useful inequalities. For simplicity, we consider only the positive component, as it is completely analogous to the negative one.

\begin{definition} The Dual Problem \newline
	Let $L \coloneqq \left\{ v \in H^1(-1,1) \,|\, v(\pm 1) \geq 0 \,\,\text{if}\,\, u(\pm 1) = 0 \right\}$.Then we define the dual problem as follows:
	\begin{equation} \label{eq:dual_variational_inequality}
		z \in L,\quad (z,v-z)_{H^1(\Omega)} \geq (-\max(0,u-u_h),v-z)_{L^2(\Omega)} \quad \forall v \in L
	\end{equation}
\end{definition}

\begin{lemma}Existence of a Unique Dual Solution \newline 
	The dual problem \eqref{eq:dual_variational_inequality} admits a unique solution $z \in H^2(\Omega)$ for all $h > 0$. Furthermore, for this solution it holds that:
	\begin{equation} \label{eq:dual_solution_nonpositive}
		z \leq 0 \quad \text{in} \quad \bar{\Omega}
	\end{equation}
	\begin{proof}
		Fixing a $h>0$, this follows from \thref{thm:unique_solution_signorini}, taking into account that $-\max(0,u-u_h) \in L^2(\Omega)$. \textcolor{red}{TODO: Adapt thm s.th. the side conditions are more lax}.
		
		For the proof of the inequality \textcolor{red}{QUESTION: In the paper this is only proved $L^2$ a.e., how might I go about proving this? (Or at least on $\partial\Omega$)}
	\end{proof}
\end{lemma}

We will now consider a solution $z \in H^2(\Omega)$ of \eqref{eq:dual_variational_inequality}, and test appropriate functions containing it in all three variational inequalities to obtain the following inequalities relating the $L^2$ to the $H^1$ error:

\begin{lemma}
	Let $z \in H^2(\Omega)$ be a solution of \eqref{eq:dual_variational_inequality}, with corresponding $u$, $u_h$ and $f$. Then the following inequalities hold:
	\begin{align}
		\norm{\max(0,u-u_h)}_{L^2(\Omega)}^2 &\leq (z,u_h-u)_{H^1(\Omega)} \\
		(f, I_h(z))_{L^2} &\leq (u, I_h(z))_{H^1(\Omega)} \\
		(f, -I_h(z))_{L^2} &\leq (u_h, -I_h(z))_{H^1(\Omega)} \\
		\norm{\max(0,u-u_h)}_{L^2(\Omega)}^2 &\leq (z-I_h(z), u_h - u)_{H^1(\Omega)} \label{eq:scalar_product_ineq_L2}
	\end{align}
	\begin{proof}
		For the first inequality, we test \eqref{eq:dual_variational_inequality} against $z+u_h-u$. This function is in $L$ \textcolor{red}{QUESTION: Why?}. The claim follows:
		\begin{align*}
		(z, u_h-u)_{H^1(\Omega)} &\geq (-\max(0,u-u_h), u_h-u)_{L^2(\Omega)} \\
		&= (\max(0,u_h-u), u-u_h)_{L^2(\Omega)} \\
		&= \norm{\max(0,u-u_h)}_{L^2(\Omega)}^2
		\end{align*}
		
		For the second one, we observe that as $u$ is a solution of \eqref{eq:variational_inequality}, it holds that $u(\pm 1) \geq 0$. Furthermore, following \eqref{eq:dual_solution_nonpositive}, we have $z(\pm 1) \leq 0$, and as the interpolate is equal at the nodes, also $I_h(z)(\pm 1) \leq 0$. If we choose $s \in \mathbb{R}_{>0}$ sufficiently small, then $u + s I_h(z) \in K$, and therefore admissible in \eqref{eq:variational_inequality}. We thus obtain:
		\begin{equation*}
		(u, sI_h(z))_{H^1(\Omega)} \geq (f,sI_h(z))_{L^2(\Omega)}
		\end{equation*}
		Dividing both sides by the positive constant $s$, the claim follows.
		
		For the third inequality, following a similar argument as before, we conclude that $u_h -I_h(z)$ is admissible for \eqref{eq:discrete_variational_inequality}. The claim follows directly.
		
		The last inequality follows by simply adding the three previous ones.
	\end{proof}
\end{lemma}

\begin{corollary}\thlabel{thm:L2_error_lemma} $L^2$ Error for the Positive Component \newline
	Let $u \in H^2(\Omega)$ be a solution of \eqref{eq:variational_inequality} for some $f \in L^2(\Omega)$ and $u_h \in V_h$ the solution of \eqref{eq:discrete_variational_inequality} for the same $f$. Then it holds:
	\begin{equation} \label{eq:L2_error_lemma}
	\norm{\max(0,u-u_h)}_{L^2(\Omega)} \leq Ch^2
	\end{equation}
	\begin{proof}
		\begin{align*}
		\norm{\max(0,u-u_h)}_{L^2(\Omega)}^2 \overset{\eqref{eq:scalar_product_ineq_L2}}&{\leq} (z-I_h(z),u_h-u)_{H^1(\Omega)} \\
		\overset{\begin{subarray}{c} \text{Cauchy-}\\ \text{Schwarz}
			\end{subarray}}&{\leq} \norm{z-I_h(z)}_{H^1(\Omega)} \norm{u_h - u}_{H^1(\Omega)} \\
			\overset{\text{\textcolor{red}{REF}} \eqref{eq:H1_error}}&{\leq} C_1h \norm{z}_{H^2(\Omega)} Ch \\
			\overset{\text{\textcolor{red}{REF}}}&{\leq} Ch^2 \norm{\max(0,u-u_h)}_{L^2(\Omega)}
		\end{align*}
		Dividing both sides by $\norm{\max(0,u-u_h)}_{L^2(\Omega)}$ yields the claim.
	\end{proof}
\end{corollary}

\begin{theorem}
	 $L^2$ Error for the Positive Component \newline
	Let $u \in H^2(\Omega)$ be a solution of \eqref{eq:variational_inequality} for some $f \in L^2(\Omega)$ and $u_h \in V_h$ the solution of \eqref{eq:discrete_variational_inequality} for the same $f$. Then it holds:
	\begin{equation}
	\norm{u-u_h}_{L^2(\Omega)} \leq Ch^2
	\end{equation}
	\begin{proof}
		This follows from the above \thref{thm:L2_error_lemma}, the analogous result for the negative component, and the following:
		\begin{align*}
		\norm{u-u_h}_{L^2(\Omega)} &\leq  \norm{u-u_h}_{L^2(\Omega \cap \{ u-u_h \geq 0\})} + \norm{u-u_h}_{L^2(\Omega \cap \{ u-u_h < 0\})} \\
		&= \norm{\max(0,u-u_h)}_{L^2(\Omega \cap \{ u-u_h \geq 0\})} + \norm{\min(0,u-u_h)}_{L^2(\Omega \cap \{ u-u_h < 0\})} \\
		&= \norm{\max(0,u-u_h)}_{L^2(\Omega)} + \norm{\min(0,u-u_h)}_{L^2(\Omega)} \\
		\overset{\eqref{eq:L2_error_lemma}}&{\leq}  Ch^2 
		\end{align*}
	\end{proof}
\end{theorem}


\chapter{Implementation in Matlab}
\section{Methods}

\chapter{Anhang}

\printbibliography{}
\end{document}



